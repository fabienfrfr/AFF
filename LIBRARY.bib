
@article{huxley_evolution_1942,
	title = {Evolution. The Modern Synthesis.},
	url = {https://www.cabdirect.org/cabdirect/abstract/19432202794},
	abstract = {T. H. {HUXLEY}, in an essay written 70 years ago, says "that whatever may be thought or said about Darwin's doctrines, or the manner in which he has propounded them, this much is certain, that, in a dozen years, the 'Origin of Species' has worked as complete a revolution in biological science as, the 'Principia' did in astronomy and it has done so because it contains an essentially new creative...},
	journaltitle = {Evolution. The Modern Synthesis.},
	author = {Huxley, J.},
	urldate = {2021-05-12},
	date = {1942},
	note = {Publisher: London: George Alien \& Unwin Ltd.},
	file = {Snapshot:/home/fabien/Zotero/storage/ST3JK2T6/19432202794.html:text/html},
}

@article{buick_when_2008,
	title = {When did oxygenic photosynthesis evolve?},
	volume = {363},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2008.0041},
	doi = {10.1098/rstb.2008.0041},
	abstract = {The atmosphere has apparently been oxygenated since the ‘Great Oxidation Event’ ca 2.4 Ga ago, but when the photosynthetic oxygen production began is debatable. However, geological and geochemical evidence from older sedimentary rocks indicates that oxygenic photosynthesis evolved well before this oxygenation event. Fluid-inclusion oils in ca 2.45 Ga sandstones contain hydrocarbon biomarkers evidently sourced from similarly ancient kerogen, preserved without subsequent contamination, and derived from organisms producing and requiring molecular oxygen. Mo and Re abundances and sulphur isotope systematics of slightly older (2.5 Ga) kerogenous shales record a transient pulse of atmospheric oxygen. As early as ca 2.7 Ga, stromatolites and biomarkers from evaporative lake sediments deficient in exogenous reducing power strongly imply that oxygen-producing cyanobacteria had already evolved. Even at ca 3.2 Ga, thick and widespread kerogenous shales are consistent with aerobic photoautrophic marine plankton, and U–Pb data from ca 3.8 Ga metasediments suggest that this metabolism could have arisen by the start of the geological record. Hence, the hypothesis that oxygenic photosynthesis evolved well before the atmosphere became permanently oxygenated seems well supported.},
	pages = {2731--2743},
	number = {1504},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Buick, Roger},
	urldate = {2021-05-12},
	date = {2008-08-27},
	note = {Publisher: Royal Society},
	file = {Texte intégral:/home/fabien/Zotero/storage/8VXK67BM/Buick - 2008 - When did oxygenic photosynthesis evolve.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/V2S2B7GJ/rstb.2008.html:text/html},
}

@article{mojzsis_evidence_1996,
	title = {Evidence for life on Earth before 3,800 million years ago},
	volume = {384},
	rights = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/384055a0},
	doi = {10.1038/384055a0},
	abstract = {{IT} is unknown when life first appeared on Earth. The earliest known microfossils (˜3,500 Myr before present) are structurally complex, and if it is assumed that the associated organisms required a long time to develop this degree of complexity, then the existence of life much earlier than this can be argued1,2. But the known examples of crustal rocks older than ˜3,500 Myr have experienced intense metamorphism, which would have obliterated any fragile microfossils contained therein. It is therefore necessary to search for geochemical evidence of past biotic activity that has been preserved within minerals that are resistant to metamorphism. Here we report ion-microprobe measure-ments of the carbon-isotope composition of carbonaceous inclusions within grains of apatite (basic calcium phosphate) from the oldest known sediment sequences—a ˜3,800-Myr-old banded iron formation from the Isua supracrustal belt, West Greenland35, and a similar formation from the nearby Akilia island that is possibly older than 3,850 Myr (ref. 3). The carbon in the carbonaceous inclusions is isotopically light, indicative of biological activity; no known abiotic process can explain the data. Unless some unknown abiotic process exists which is able both to create such isotopically light carbon and then selectively incorporate it into apatite grains, our results provide evidence for the emergence of life on Earth by at least 3,800 Myr before present.},
	pages = {55--59},
	number = {6604},
	journaltitle = {Nature},
	author = {Mojzsis, S. J. and Arrhenius, G. and {McKeegan}, K. D. and Harrison, T. M. and Nutman, A. P. and Friend, C. R. L.},
	urldate = {2021-05-12},
	date = {1996-11},
	langid = {english},
	note = {Number: 6604
Publisher: Nature Publishing Group},
	file = {Snapshot:/home/fabien/Zotero/storage/9AITVTEV/384055a0.html:text/html},
}

@article{hedges_molecular_2004,
	title = {A molecular timescale of eukaryote evolution and the rise of complex multicellular life},
	volume = {4},
	issn = {1471-2148},
	url = {https://doi.org/10.1186/1471-2148-4-2},
	doi = {10.1186/1471-2148-4-2},
	abstract = {The pattern and timing of the rise in complex multicellular life during Earth's history has not been established. Great disparity persists between the pattern suggested by the fossil record and that estimated by molecular clocks, especially for plants, animals, fungi, and the deepest branches of the eukaryote tree. Here, we used all available protein sequence data and molecular clock methods to place constraints on the increase in complexity through time.},
	pages = {2},
	number = {1},
	journaltitle = {{BMC} Evolutionary Biology},
	shortjournal = {{BMC} Evolutionary Biology},
	author = {Hedges, S. Blair and Blair, Jaime E. and Venturi, Maria L. and Shoe, Jason L.},
	urldate = {2021-05-12},
	date = {2004-01-28},
	keywords = {Fossil Calibration, Molecular Clock, Primary Calibration, Protein Sequence Data, Secondary Calibration},
	file = {Full Text PDF:/home/fabien/Zotero/storage/EB6GHIQ5/Hedges et al. - 2004 - A molecular timescale of eukaryote evolution and t.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/U3I5E6EL/1471-2148-4-2.html:text/html},
}

@article{knoll_ediacaran_2006,
	title = {The Ediacaran Period: a new addition to the geologic time scale},
	volume = {39},
	issn = {1502-3931},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/00241160500409223},
	doi = {https://doi.org/10.1080/00241160500409223},
	shorttitle = {The Ediacaran Period},
	abstract = {The International Union of Geological Sciences has approved a new addition to the geologic time scale: the Ediacaran Period. The Ediacaran is the first Proterozoic period to be recognized on the basis of chronostratigraphic criteria and the first internationally ratified, chronostratigraphically defined period of any age to be introduced in more than a century. In accordance with procedures established by the International Commission on Stratigraphy, the base of the Ediacaran Period is defined by a Global Stratotype Section and Point ({GSSP}) placed at the base of the Nuccaleena Formation cap carbonate directly above glacial diamictites and associated facies at Enorama Creek in the Flinders Ranges of South Australia. Its top is defined by the initial {GSSP} of the Cambrian Period. The new Ediacaran Period encompasses a distinctive interval of Earth history that is bounded both above and below by equally distinctive intervals. Both chemostratigraphic and biostratigraphic data indicate that the subdivision of the period into two or more series is feasible, and this should be a primary objective of continuing work by the Ediacaran Subcommission of the {ICS}.},
	pages = {13--30},
	number = {1},
	journaltitle = {Lethaia},
	author = {Knoll, Andrew and Walter, Malcolm and Narbonne, Guy and Christie Blick, Nicholas},
	urldate = {2021-05-12},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/00241160500409223},
	keywords = {Ediacaran, Geologic time scale, global stratotype section and point, Proterozoic},
	file = {Snapshot:/home/fabien/Zotero/storage/8TZNNANI/00241160500409223.html:text/html;Version soumise:/home/fabien/Zotero/storage/4UGLV2K5/Knoll et al. - 2006 - The Ediacaran Period a new addition to the geolog.pdf:application/pdf},
}

@article{bonner_origins_1998,
	title = {The origins of multicellularity},
	volume = {1},
	rights = {Copyright © 1998 Wiley‐Liss, Inc.},
	issn = {1520-6602},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291520-6602%281998%291%3A1%3C27%3A%3AAID-INBI4%3E3.0.CO%3B2-6},
	doi = {https://doi.org/10.1002/(SICI)1520-6602(1998)1:1<27::AID-INBI4>3.0.CO;2-6},
	abstract = {There is great interest in the invention of multicellularity because it is one of the major transitions during the course of early evolution.1 Most of the emphasis has been on why it occurred. For instance, recently Gerhart and Kirschner2 have argued that a multicellular organism has gained the advantage of a unicellular ancestor because it can more effectively shield itself from the vagaries of the environment by producing its own internal environment. In broader terms, this is Dawkins'3 argument that a competitively effective way of carrying the genes from one generation to the next is by building a complex soma that safely sees to it that the germ plasm survives. © 1998 Wiley-Liss, Inc.},
	pages = {27--36},
	number = {1},
	journaltitle = {Integrative Biology: Issues, News, and Reviews},
	author = {Bonner, John Tyler},
	urldate = {2021-05-12},
	date = {1998},
	langid = {english},
	keywords = {cell differentiation, early evolution, multicellularity},
	file = {Snapshot:/home/fabien/Zotero/storage/NWRPCDSC/(SICI)1520-6602(1998)1127AID-INBI43.0.html:text/html},
}

@article{bartlett_defining_2020,
	title = {Defining Lyfe in the Universe: From Three Privileged Functions to Four Pillars},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2075-1729/10/4/42},
	doi = {10.3390/life10040042},
	shorttitle = {Defining Lyfe in the Universe},
	abstract = {Motivated by the need to paint a more general picture of what life is\&mdash;and could be\&mdash;with respect to the rest of the phenomena of the universe, we propose a new vocabulary for astrobiological research. Lyfe is defined as any system that fulfills all four processes of the living state, namely: dissipation, autocatalysis, homeostasis, and learning. Life is defined as the instance of lyfe that we are familiar with on Earth, one that uses a specific organometallic molecular toolbox to record information about its environment and achieve dynamical order by dissipating certain planetary disequilibria. This new classification system allows the astrobiological community to more clearly define the questions that propel their research\&mdash;e.g., whether they are developing a historical narrative to explain the origin of life (on Earth), or a universal narrative for the emergence of lyfe, or whether they are seeking signs of life specifically, or lyfe at large across the universe. While the concept of \&ldquo;life as we don\&rsquo;t know it\&rdquo; is not new, the four pillars of lyfe offer a novel perspective on the living state that is indifferent to the particular components that might produce it.},
	pages = {42},
	number = {4},
	journaltitle = {Life},
	author = {Bartlett, Stuart and Wong, Michael L.},
	urldate = {2021-05-12},
	date = {2020-04},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {astrobiology, definition of life, mechanotroph, origin of life},
	file = {Full Text PDF:/home/fabien/Zotero/storage/WY2QEZU9/Bartlett et Wong - 2020 - Defining Lyfe in the Universe From Three Privileg.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/K34UI4AT/42.html:text/html},
}

@article{maturanat_what_nodate,
	title = {What the Frog's Eye Tells the Frog's Brain},
	abstract = {In this paper, we analyze the activity of single fibers in the optic nerve of a frog. Our method is to find what sort of stimulus causes the largest activity in one nerve fiber and then what is the exciting aspect of that stimulus such that variations in everything else cause little change in the response. It has been known for the past 20 years that each fiber is connected not to a few rods and cones in the retina but to very many over a fair area. Our results show that for the most part within that area, it is not the light intensity itself but rather the pattern of local variation of intensity that is the exciting factor. There are four types of fibers, each type concerned with a different sort of pattern. Each type is uniformly distributed over the whole retina of the frog. Thus, there are four distinct parallel distributed channels whereby the frog's eye informs his brain about the visual image in terms of local pattern independent of average illumination. We describe the patterns and show the functional and anatomical separation of the channels. This work has been done on the frog, and our interpretation applies only to the frog.},
	pages = {12},
	journaltitle = {{PROCEEDINGS} {OF} {THE} {IRE}},
	author = {Maturanat, H R and Pitts, W H},
	langid = {english},
	file = {Maturanat et Pitts - What the Frog's Eye Tells the Frog's Brain.pdf:/home/fabien/Zotero/storage/K8KXTWRP/Maturanat et Pitts - What the Frog's Eye Tells the Frog's Brain.pdf:application/pdf},
}

@article{stuart_action_1997,
	title = {Action potential initiation and backpropagation in neurons of the mammalian {CNS}},
	volume = {20},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(96)10075-8},
	doi = {10.1016/S0166-2236(96)10075-8},
	pages = {125--131},
	number = {3},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Stuart, Greg and Spruston, Nelson and Sakmann, Bert and Häusser, Michael},
	urldate = {2021-05-14},
	date = {1997-03-01},
	pmid = {9061867},
	note = {Publisher: Elsevier},
	keywords = {active propogation, central nervous system, dendrites, neurons, sodium channels, synaptic integration},
	file = {Snapshot:/home/fabien/Zotero/storage/N6TCN3ZT/S0166-2236(96)10075-8.html:text/html},
}

@article{konda_actor-critic_nodate,
	title = {Actor-Critic Algorithms},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses {TD} learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	pages = {7},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	langid = {english},
	file = {Konda et Tsitsiklis - Actor-Critic Algorithms.pdf:/home/fabien/Zotero/storage/92TL3Z66/Konda et Tsitsiklis - Actor-Critic Algorithms.pdf:application/pdf},
}

@article{konda_onactor-critic_2003,
	title = {{OnActor}-Critic Algorithms},
	volume = {42},
	issn = {0363-0129, 1095-7138},
	url = {http://epubs.siam.org/doi/10.1137/S0363012901385691},
	doi = {10.1137/S0363012901385691},
	abstract = {In this article, we propose and analyze a class of actor-critic algorithms. These are two-time-scale algorithms in which the critic uses temporal diﬀerence learning with a linearly parameterized approximation architecture, and the actor is updated in an approximate gradient direction, based on information provided by the critic. We show that the features for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor. We study actor-critic algorithms for Markov decision processes with Polish state and action spaces. We state and prove two results regarding their convergence.},
	pages = {1143--1166},
	number = {4},
	journaltitle = {{SIAM} Journal on Control and Optimization},
	shortjournal = {{SIAM} J. Control Optim.},
	author = {Konda, Vijay R. and Tsitsiklis, John N.},
	urldate = {2021-05-14},
	date = {2003-01},
	langid = {english},
	file = {Konda et Tsitsiklis - 2003 - OnActor-Critic Algorithms.pdf:/home/fabien/Zotero/storage/DJGQSHKU/Konda et Tsitsiklis - 2003 - OnActor-Critic Algorithms.pdf:application/pdf},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: The third generation of neural network models},
	volume = {10},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	shorttitle = {Networks of spiking neurons},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on {McCulloch} Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	pages = {1659--1671},
	number = {9},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Maass, Wolfgang},
	urldate = {2021-06-23},
	date = {1997-12-01},
	langid = {english},
	keywords = {Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron},
	file = {ScienceDirect Snapshot:/home/fabien/Zotero/storage/254JDRY8/S0893608097000117.html:text/html},
}

@book{matthews_neurobiology_2000,
	title = {Neurobiology: Molecules, Cells and Systems},
	isbn = {978-0-632-04496-2},
	shorttitle = {Neurobiology},
	abstract = {Visit the Neurobiology Website at: www.blackwellpublishing.com/{matthewsAs} the second edition of a very successful neurobiology book, this text covers a range from molecules to systems, and uses various systems to illustrate each major concept. In addition to the text, this title offers a companion website, which features animations of difficult concepts, online assignments and practice exams, as well as all text figures in an easy to download format.Four colour throughout. New chapter on hypothalamic function with focus on circadian rhythms. More clinical correlation. Improved illustration quality and quantity. Comprehensive text with excellent coverage of subjects from molecules to systems. Use of systems to illustrate each major concept.},
	pagetotal = {604},
	publisher = {Wiley},
	author = {Matthews, Gary G.},
	date = {2000-12-27},
	langid = {english},
	keywords = {Medical / Neuroscience},
}

@book{anctil_dawn_2015,
	location = {Montreal ; Kingston ; London ; Chicago},
	title = {Dawn of the neuron: the early struggles to trace the origin of nervous systems},
	isbn = {978-0-7735-9732-7 978-0-7735-9733-4},
	shorttitle = {Dawn of the neuron},
	abstract = {"In science, sometimes it is best to keep things simple. Initially discrediting the discovery of neurons in jellyfish, mid-nineteenth-century scientists grouped jellyfish, comb-jellies, hydra, and sea anemones together under one term - "coelenterates"--And deemed these animals too similar to plants to warrant a nervous system. In Dawn of the Neuron, Michel Anctil shows how Darwin's theory of evolution completely eradicated this idea and cleared the way for the modern study of the neuron. Once zoologists accepted the notion that varying levels of animal complexity could evolve, they began to use simple-structured creatures such as coelenterates and sponges to understand the building blocks of more complicated nervous systems. Dawn of the Neuron provides fascinating insights into the labours and lives of scientists who studied coelenterate nervous systems over several generations, and who approached the puzzling origin of the first nerve cells through the process outlined in evolutionary theory. Anctil also reveals how these scientists, who were willing to embrace improved and paradigm-changing scientific methods, still revealed their cultural backgrounds, their societal biases, and their attachments to schools of thought and academic traditions while presenting their ground-breaking work. Their attitudes toward the neuron doctrine - where neurons are individual, self-contained cells - proved decisive in the exploration of how neurons first emerged. Featuring photographs and historical sketches to illustrate this quest for knowledge, Dawn of the Neuron is a remarkably in-depth exploration of the link between Darwin's theory of evolution and pioneering studies and understandings of the first evolved nervous systems, "--Amazon.com},
	pagetotal = {1},
	publisher = {{McGill}-Queen's University Press},
	author = {Anctil, Michel},
	date = {2015},
	note = {{OCLC}: 909956847},
	keywords = {Anatomy, Biological Evolution, Cnidaria, cytology, history, History, Life Sciences Human Anatomy \& Physiology, {MEDICAL}, Nervous system, Nervous System, Neuroanatomy, Neurons, Neurophysiology, {SCIENCE}},
}

@incollection{child_nervous_1921,
	location = {Chicago, {IL}, {US}},
	title = {Nervous centralization and cephalization in evolution},
	abstract = {The terms "centralization" and "central nervous system" possess of course a morphological and a physiological significance. Morphologically, centralization is the process, both ontogenetic and phylogenetic, of localization and aggregation of the chief mass of nervous tissue in a particular region of the body. The degree of integration corresponding to a particular degree of centralization or cephalization in a particular group depends upon the hereditary potentialities of the protoplasm concerned as regards the development of nervous structure and function. This chapter looks at centralization in relation to axiation, progressive centralization and cephalization in various groups, the "stepladder" type of nervous system in the invertebrates, and the position of peripheral receptors. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {142--154},
	booktitle = {The origin and development of the nervous system: From a physiological viewpoint},
	publisher = {University of Chicago Press},
	author = {Child, Charles Manning},
	date = {1921},
	doi = {10.1037/10916-009},
	keywords = {Central Nervous System, Morphology, Neural Development, Peripheral Nervous System, Physiology, Theory of Evolution},
	file = {Snapshot:/home/fabien/Zotero/storage/NBEI8C7X/2006-03519-009.html:text/html},
}

@article{chitwood_imitation_2014,
	title = {Imitation, Genetic Lineages, and Time Influenced the Morphological Evolution of the Violin},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0109229},
	doi = {10.1371/journal.pone.0109229},
	abstract = {Violin design has been in flux since the production of the first instruments in 16th century Italy. Numerous innovations have improved the acoustical properties and playability of violins. Yet, other attributes of the violin affect its performance less, and with fewer constraints, are potentially more sensitive to historical vagaries unrelated to quality. Although the coarse shape of violins is integral to their design, details of the body outline can vary without significantly compromising sound quality. What can violin shapes tell us about their makers and history, including the degree that luthiers have influenced each other and the evolution of complex morphologies over time? Here, I provide an analysis of morphological evolution in the violin family, sampling the body shapes of over 9,000 instruments over 400 years of history. Specific shape attributes, which discriminate instruments produced by different luthiers, strongly correlate with historical time. Linear discriminant analysis reveals luthiers who likely copied the outlines of their instruments from others, which historical accounts corroborate. Clustering of averaged violin shapes places luthiers into four major groups, demonstrating a handful of discrete shapes predominate in most instruments. Violin shapes originating from multi-generational luthier families tend to cluster together, and familial origin is a significant explanatory factor of violin shape. Together, the analysis of four centuries of violin shapes demonstrates not only the influence of history and time leading to the modern violin, but widespread imitation and the transmission of design by human relatedness.},
	pages = {e109229},
	number = {10},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Chitwood, Daniel H.},
	urldate = {2021-06-25},
	date = {2014-10-08},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Acoustics, Cities, Evolutionary genetics, Evolutionary processes, Fourier analysis, Hierarchical clustering, Linear discriminant analysis, Principal component analysis},
	file = {Full Text PDF:/home/fabien/Zotero/storage/UNLZAHH6/Chitwood - 2014 - Imitation, Genetic Lineages, and Time Influenced t.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/SHRDCSSL/article.html:text/html},
}

@article{rogers_natural_2008,
	title = {Natural selection and cultural rates of change},
	volume = {105},
	doi = {10.1073/pnas.0711802105},
	abstract = {It has been claimed that a meaningful theory of cultural evolution is not possible because human beliefs and behaviors do not follow predictable patterns. However, theoretical models of cultural transmission and observations of the development of societies suggest that patterns in cultural evolution do occur. Here, we analyze whether two sets of related cultural traits, one tested against the environment and the other not, evolve at different rates in the same populations. Using functional and symbolic design features for Polynesian canoes, we show that natural selection apparently slows the evolution of functional structures, whereas symbolic designs differentiate more rapidly. This finding indicates that cultural change, like genetic evolution, can follow theoretically derived patterns.

• canoe design
• cultural evolution
• Polynesia
• signatures of selection},
	pages = {3416--20},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rogers, Deborah and Ehrlich, Paul},
	date = {2008-04-01},
	file = {Texte intégral:/home/fabien/Zotero/storage/HD6YWMUR/Rogers et Ehrlich - 2008 - Natural selection and cultural rates of change.pdf:application/pdf},
}

@online{noauthor_nasa_nodate,
	title = {{NASA} - Life's Working Definition: Does It Work?},
	url = {https://www.nasa.gov/vision/universe/starsgalaxies/life%27s_working_definition.html},
	shorttitle = {{NASA} - Life's Working Definition},
	type = {Feature Articles},
	urldate = {2021-06-25},
	langid = {english},
	note = {Publisher: Brian Dunbar},
	file = {Snapshot:/home/fabien/Zotero/storage/L64BVA38/life's_working_definition.html:text/html},
}

@article{fredens_total_2019,
	title = {Total synthesis of Escherichia coli with a recoded genome},
	volume = {569},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7039709/},
	doi = {10.1038/s41586-019-1192-5},
	abstract = {Nature uses 64 codons to encode the synthesis of proteins from the genome, and chooses 1 sense codon—out of up to 6 synonyms—to encode each amino acid. Synonymous codon choice has diverse and important roles, and many synonymous substitutions are detrimental. Here we demonstrate that the number of codons used to encode the canonical amino acids can be reduced, through the genome-wide substitution of target codons by defined synonyms. We create a variant of Escherichia coli with a four-megabase synthetic genome through a high-fidelity convergent total synthesis. Our synthetic genome implements a defined recoding and refactoring scheme—with simple corrections at just seven positions—to replace every known occurrence of two sense codons and a stop codon in the genome. Thus, we recode 18,214 codons to create an organism with a 61-codon genome; this organism uses 59 codons to encode the 20 amino acids, and enables the deletion of a previously essential transfer {RNA}.},
	pages = {514--518},
	number = {7757},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Fredens, Julius and Wang, Kaihang and de la Torre, Daniel and Funke, Louise F. H. and Robertson, Wesley E. and Christova, Yonka and Chia, Tiongsun and Schmied, Wolfgang H. and Dunkelmann, Daniel and Beranek, Vaclav and Uttamapinant, Chayasith and Llamazares, Andres Gonzalez and Elliott, Thomas S. and Chin, Jason W.},
	urldate = {2021-06-25},
	date = {2019-05-01},
	pmid = {31092918},
	pmcid = {PMC7039709},
	file = {PubMed Central Full Text PDF:/home/fabien/Zotero/storage/DZBJ3X9J/Fredens et al. - 2019 - Total synthesis of Escherichia coli with a recoded.pdf:application/pdf},
}

@article{yaeger_computational_1995,
	title = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {PolyWorld}: Life in a New Context},
	shorttitle = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {PolyWorld}},
	abstract = {This paper discusses a computer model of living organisms and the ecology they exist in called {PolyWorld}. {PolyWorld} attempts to bring together all the principle components of real living systems into a single artificial (man-made) living system. {PolyWorld} brings together biologically motivated genetics, simple simulated physiologies and metabolisms, Hebbian learning in arbitrary neural network architectures, a visual perceptive mechanism, and a suite of primitive behaviors in artificial organisms grounded in an ecology just complex enough to foster speciation and inter-species competition. Predation, mimicry, sexual reproduction, and even communication are all supported in a straightforward fashion. The resulting survival strategies, both individual and group, are purely emergent, as are the functionalities embodied in their neural network "brains". Complex behaviors resulting from the simulated neural activity are unpredictable, and change as natural selection acts over multiple generations. In many ways, {PolyWorld} may be thought of as a sort of electronic primordial soup experiment, in the vein of Urey and Miller's [33] classic experiment, only commencing at a much higher level of organization. While one could claim that Urey and Miller really just threw a bunch of ingredients in a pot and watched to see what happened, the reason these men made a contribution to science rather than ratatouille is that they put the right ingredients in the right pot ... and watched to see what happened. Here we start with software-coded genetics and various simple nerve cells (lightsensitive, motor, and unspecified neuronal) as the ingredients, and place them in a competitive ecological crucible which subjects them to an internally consistent physics and the process of natural selectio...},
	author = {Yaeger, Larry},
	date = {1995-03-23},
	file = {Full Text PDF:/home/fabien/Zotero/storage/FII6GSWT/Yaeger - 1995 - Computational Genetics, Physiology, Metabolism, Ne.pdf:application/pdf},
}

@article{kemeny_theory_1967,
	title = {Theory of Self-Reproducing Automata. John von Neumann. Edited by Arthur W. Burks. University of Illinois Press, Urbana, 1966. 408 pp., illus. \$10},
	volume = {157},
	rights = {© 1967},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/157/3785/180.1},
	doi = {10.1126/science.157.3785.180},
	pages = {180--180},
	number = {3785},
	journaltitle = {Science},
	author = {Kemeny, John G.},
	urldate = {2021-06-25},
	date = {1967-07-14},
	langid = {english},
	note = {Publisher: American Association for the Advancement of Science
Section: Book Reviews},
	file = {Snapshot:/home/fabien/Zotero/storage/IZC5TNRY/180.html:text/html},
}

@article{noauthor_mathematical_1970,
	title = {Mathematical Games - The fantastic combinations of John Conway's new solitaire game "life" - M. Gardner - 1970},
	pages = {6},
	date = {1970},
	langid = {english},
	file = {1970 - Mathematical Games - The fantastic combinations of.pdf:/home/fabien/Zotero/storage/PW5S7NIM/1970 - Mathematical Games - The fantastic combinations of.pdf:application/pdf},
}

@incollection{wainwright_conways_2010,
	location = {London},
	title = {Conway’s Game of Life: Early Personal Recollections},
	isbn = {978-1-84996-217-9},
	url = {https://doi.org/10.1007/978-1-84996-217-9_2},
	shorttitle = {Conway’s Game of Life},
	abstract = {When the October 1970 issue of Scientific American arrived, I had no idea the extent to which Martin Gardner’s article in that issue would affect my life. As long as I can remember, my custom would be to seek out the Mathematical Games column in search for Gardner’s latest topic with the usual reader challenges. My first reaction to that particular article introducing a new pastime titled “The fantastic combinations of John Conway’s new solitaire game ‘life''' was only mildly interesting. A couple of days later, still curious about the outcome of random patterns, I located an old checkerboard and a small jarful of pennies to investigate this new game.},
	pages = {11--16},
	booktitle = {Game of Life Cellular Automata},
	publisher = {Springer},
	author = {Wainwright, Robert},
	editor = {Adamatzky, Andrew},
	urldate = {2021-06-25},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-1-84996-217-9_2},
}

@book{berlekamp_winning_2003,
	location = {New York},
	edition = {2},
	title = {Winning Ways for Your Mathematical Plays},
	isbn = {978-0-429-48732-3},
	abstract = {In the quarter of a century since three mathematicians and game theorists collaborated to create Winning Ways for Your Mathematical Plays, the book has become the definitive work on the subject of mathematical games. Now carefully revised and broken down into four volumes to accommodate new developments, the Second Edition retains the original's wealth of wit and wisdom. The authors' insightful strategies, blended with their witty and irreverent style, make reading a profitable pleasure. In Volume 2, the authors have a Change of Heart, bending the rules established in Volume 1 to apply them to games such as Cut-cake and Loopy Hackenbush. From the Table of Contents: - If You Can't Beat 'Em, Join 'Em! - Hot Bottles Followed by Cold Wars - Games Infinite and Indefinite - Games Eternal--Games Entailed - Survival in the Lost World},
	pagetotal = {212},
	publisher = {A K Peters/{CRC} Press},
	author = {Berlekamp, Elwyn R. and Conway, John H. and Guy, Richard K.},
	date = {2003-03-01},
	doi = {10.1201/9780429487323},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urldate = {2021-06-25},
	date = {1943-12-01},
	langid = {english},
}

@article{frs_liii_1901,
	title = {{LIII}. On lines and planes of closest fit to systems of points in space},
	volume = {2},
	issn = {1941-5982},
	url = {https://doi.org/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	pages = {559--572},
	number = {11},
	journaltitle = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {F.R.S, Karl Pearson},
	urldate = {2021-06-25},
	date = {1901-11-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14786440109462720},
	file = {Version soumise:/home/fabien/Zotero/storage/VJUUTJ8B/F.R.S - 1901 - LIII. On lines and planes of closest fit to system.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/T8PED4KG/14786440109462720.html:text/html},
}

@book{vapnik_nature_2000,
	location = {New York},
	edition = {2},
	title = {The Nature of Statistical Learning Theory},
	isbn = {978-0-387-98780-4},
	url = {https://www.springer.com/gp/book/9780387987804},
	series = {Information Science and Statistics},
	abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and {SVM} techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader {AT}\&T Labs-Research and Professor of London University. He is one of the founders of},
	publisher = {Springer-Verlag},
	author = {Vapnik, Vladimir},
	urldate = {2021-06-25},
	date = {2000},
	langid = {english},
	doi = {10.1007/978-1-4757-3264-1},
	file = {Snapshot:/home/fabien/Zotero/storage/4MVQQ8QR/9780387987804.html:text/html},
}

@incollection{bock_clustering_2007,
	location = {Berlin, Heidelberg},
	title = {Clustering Methods: A History of k-Means Algorithms},
	isbn = {978-3-540-73560-1},
	url = {https://doi.org/10.1007/978-3-540-73560-1_15},
	series = {Studies in Classification, Data Analysis, and Knowledge Organization},
	shorttitle = {Clustering Methods},
	abstract = {This paper surveys some historical issues related to the well-known k-means algorithm in cluster analysis. It shows to which authors the different versions of this algorithm can be traced back, and which were the underlying applications. We sketch various generalizations (with references also to Diday’s work) and thereby underline the usefulness of the k-means approach in data analysis.},
	pages = {161--172},
	booktitle = {Selected Contributions in Data Analysis and Classification},
	publisher = {Springer},
	author = {Bock, Hans-Hermann},
	editor = {Brito, Paula and Cucumel, Guy and Bertrand, Patrice and de Carvalho, Francisco},
	urldate = {2021-06-25},
	date = {2007},
	langid = {english},
	doi = {10.1007/978-3-540-73560-1_15},
	keywords = {Class Centroid, Class Prototype, Cluster Criterion, Comptes Rendus Acad, Supply Point},
}

@article{hinton_learning_2007,
	title = {Learning multiple layers of representation},
	volume = {11},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661307002173},
	doi = {10.1016/j.tics.2007.09.004},
	abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
	pages = {428--434},
	number = {10},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Hinton, Geoffrey E.},
	urldate = {2021-06-25},
	date = {2007-10-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/fabien/Zotero/storage/VJEH9TYZ/S1364661307002173.html:text/html},
}

@article{morris_do_1999,
	title = {D.O. Hebb: The Organization of Behavior, Wiley: New York; 1949},
	volume = {50},
	issn = {0361-9230},
	doi = {10.1016/s0361-9230(99)00182-3},
	shorttitle = {D.O. Hebb},
	pages = {437},
	number = {5},
	journaltitle = {Brain Research Bulletin},
	shortjournal = {Brain Res Bull},
	author = {Morris, R. G.},
	date = {1999-12},
	pmid = {10643472},
	keywords = {Animals, Behavior, Animal, Cognitive Science, History, 20th Century, Neurosciences, Publishing},
}

@book{minsky_perceptrons_1988,
	location = {Cambridge, Mass},
	edition = {Expanded ed},
	title = {Perceptrons: an introduction to computational geometry},
	isbn = {978-0-262-63111-2},
	shorttitle = {Perceptrons},
	pagetotal = {292},
	publisher = {{MIT} Press},
	author = {Minsky, Marvin and Papert, Seymour},
	date = {1988},
	keywords = {Data processing, Geometry, Machine learning, Parallel processing (Electronic computers), Perceptrons},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2021-06-25},
	date = {1986-10},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6088
Primary\_atype: Research
Publisher: Nature Publishing Group},
	file = {Snapshot:/home/fabien/Zotero/storage/VAI6WC3F/323533a0.html:text/html},
}

@article{stuart_action_1997-1,
	title = {Action potential initiation and backpropagation in neurons of the mammalian {CNS}},
	volume = {20},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(96)10075-8},
	doi = {10.1016/S0166-2236(96)10075-8},
	pages = {125--131},
	number = {3},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Stuart, Greg and Spruston, Nelson and Sakmann, Bert and Häusser, Michael},
	urldate = {2021-06-25},
	date = {1997-03-01},
	pmid = {9061867},
	note = {Publisher: Elsevier},
	keywords = {active propogation, central nervous system, dendrites, neurons, sodium channels, synaptic integration},
	file = {Snapshot:/home/fabien/Zotero/storage/PLP7WGBL/S0166-2236(96)10075-8.html:text/html},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {0022-3751},
	doi = {10.1113/jphysiol.1968.sp008455},
	abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers ({II} and the upper two-thirds of {III}) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer {III}, and in {IV} A and {IV} B. In layer {IV} B they form a large proportion of the population, whereas complex cells are rare. In layers {IV} A and {IV} B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer {IV} most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and {VI} contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer {IV}, the higher orders in the upper and lower layers.},
	pages = {215--243},
	number = {1},
	journaltitle = {The Journal of Physiology},
	shortjournal = {J Physiol},
	author = {Hubel, D. H. and Wiesel, T. N.},
	date = {1968-03},
	pmid = {4966457},
	pmcid = {PMC1557912},
	keywords = {Animals, Color Perception, Evoked Potentials, Haplorhini, Light, Motion Perception, Occipital Lobe, Retina, Vision, Ocular, Visual Fields},
	file = {Texte intégral:/home/fabien/Zotero/storage/VAZ74Q99/Hubel et Wiesel - 1968 - Receptive fields and functional architecture of mo.pdf:application/pdf},
}

@article{ciresan_multi-column_2012,
	title = {Multi-column Deep Neural Networks for Image Classification},
	url = {http://arxiv.org/abs/1202.2745},
	abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive {MNIST} handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	journaltitle = {{arXiv}:1202.2745 [cs]},
	author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Juergen},
	urldate = {2021-06-25},
	date = {2012-02-13},
	eprinttype = {arxiv},
	eprint = {1202.2745},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/836NDNCH/Cireşan et al. - 2012 - Multi-column Deep Neural Networks for Image Classi.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/QFTD3ZQH/1202.html:text/html},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	pages = {279--292},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	urldate = {2021-06-25},
	date = {1992-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/fabien/Zotero/storage/AFU92BXF/Watkins et Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{konda_actor-critic_2000,
	title = {Actor-critic algorithms},
	abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2002. Includes bibliographical references (leaves 143-147). Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single "overall" reward. In these formulations, finding an optimal decision policy involves computing a certain "value function" which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. For many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1) Approximate the value function. (2) Restrict the search for a good policy to a smaller family of policies. In this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. (cont.) We propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference ({TD}) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. To compute the rate of convergence ({ROC}) of our algorithms, we develop a general theory of the {ROC} of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the {ROC} of {TD} learning and compare it with related methods such as Least Squares {TD} ({LSTD}). We study the effect of the basis functions used for linear function approximation on the {ROC} of {TD}. We also show that the {ROC} of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. Finally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the "natural" basis functions are used for the critic, the rate of convergence of the actor- critic algorithms is the same as that of certain stochastic gradient descent algorithms ... by Vijaymohan Konda. Ph.D.},
	author = {Konda, Vijay and Gao, Vijaymohan},
	date = {2000-01-01},
	file = {Full Text PDF:/home/fabien/Zotero/storage/D5VR3N7N/Konda et Gao - 2000 - Actor-critic algorithms.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2021-06-25},
	date = {1997-11-15},
	file = {Full Text PDF:/home/fabien/Zotero/storage/HFGIXR7J/Hochreiter et Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/7YGE9IDJ/Long-Short-Term-Memory.html:text/html},
}

@article{stanley_evolving_2002,
	title = {Evolving Neural Networks through Augmenting Topologies},
	volume = {10},
	issn = {1063-6560},
	doi = {10.1162/106365602320169811},
	abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, {NeuroEvolution} of Augmenting Topologies ({NEAT}), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. {NEAT} is also an important contribution to {GAs} because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
	pages = {99--127},
	number = {2},
	journaltitle = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	date = {2002-06},
	note = {Conference Name: Evolutionary Computation},
	keywords = {competing conventions, Genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	file = {IEEE Xplore Abstract Record:/home/fabien/Zotero/storage/LJV6QYRF/6790655.html:text/html},
}

@article{rakison_infants_2008,
	title = {Do infants possess an evolved spider-detection mechanism?},
	volume = {107},
	issn = {1873-7838(Electronic),0010-0277(Print)},
	doi = {10.1016/j.cognition.2007.07.022},
	abstract = {Previous studies with various non-human animals have revealed that they possess an evolved predator recognition mechanism that specifies the appearance of recurring threats. We used the preferential looking and habituation paradigms in three experiments to investigate whether 5-month-old human infants have a perceptual template for spiders that generalizes to real-world images of spiders. A fourth experiment assessed whether 5-month-olds have a perceptual template for a non-threatening biological stimulus (i.e., a flower). The results supported the hypothesis that humans, like other species, may possess a cognitive mechanism for detecting specific animals that were potentially harmful throughout evolutionary history. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {381--393},
	number = {1},
	journaltitle = {Cognition},
	author = {Rakison, David H. and Derringer, Jaime},
	date = {2008},
	note = {Place: Netherlands
Publisher: Elsevier Science},
	keywords = {Arachnida, Cognitive Development, Infant Development, Recognition (Learning)},
	file = {Snapshot:/home/fabien/Zotero/storage/MHR6J8QM/2008-03366-020.html:text/html},
}

@article{rasch_about_2013,
	title = {About Sleep's Role in Memory},
	volume = {93},
	issn = {0031-9333},
	url = {https://journals.physiology.org/doi/full/10.1152/physrev.00032.2012},
	doi = {10.1152/physrev.00032.2012},
	abstract = {Over more than a century of research has established the fact that sleep benefits the retention of memory. In this review we aim to comprehensively cover the field of “sleep and memory” research by providing a historical perspective on concepts and a discussion of more recent key findings. Whereas initial theories posed a passive role for sleep enhancing memories by protecting them from interfering stimuli, current theories highlight an active role for sleep in which memories undergo a process of system consolidation during sleep. Whereas older research concentrated on the role of rapid-eye-movement ({REM}) sleep, recent work has revealed the importance of slow-wave sleep ({SWS}) for memory consolidation and also enlightened some of the underlying electrophysiological, neurochemical, and genetic mechanisms, as well as developmental aspects in these processes. Specifically, newer findings characterize sleep as a brain state optimizing memory consolidation, in opposition to the waking brain being optimized for encoding of memories. Consolidation originates from reactivation of recently encoded neuronal memory representations, which occur during {SWS} and transform respective representations for integration into long-term memory. Ensuing {REM} sleep may stabilize transformed memories. While elaborated with respect to hippocampus-dependent memories, the concept of an active redistribution of memory representations from networks serving as temporary store into long-term stores might hold also for non-hippocampus-dependent memory, and even for nonneuronal, i.e., immunological memories, giving rise to the idea that the offline consolidation of memory during sleep represents a principle of long-term memory formation established in quite different physiological systems.},
	pages = {681--766},
	number = {2},
	journaltitle = {Physiological Reviews},
	author = {Rasch, Björn and Born, Jan},
	urldate = {2021-06-25},
	date = {2013-04-01},
	note = {Publisher: American Physiological Society},
	file = {Full Text PDF:/home/fabien/Zotero/storage/TH4YLPP4/Rasch et Born - 2013 - About Sleep's Role in Memory.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/25PPFUEM/physrev.00032.html:text/html},
}

@online{noauthor_activation_2020,
	title = {Activation Functions {\textbar} Fundamentals Of Deep Learning},
	url = {https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/},
	abstract = {An introduction to activation functions. This article describes when to use which type of activation function and fundamentals of deep learning.},
	titleaddon = {Analytics Vidhya},
	urldate = {2021-06-25},
	date = {2020-01-29},
	file = {Snapshot:/home/fabien/Zotero/storage/RHJX58LI/fundamentals-deep-learning-activation-functions-when-to-use-them.html:text/html},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in Neural Networks: An Overview},
	volume = {61},
	issn = {08936080},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	shorttitle = {Deep Learning in Neural Networks},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	urldate = {2021-06-25},
	date = {2015-01},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/7AJAD3VY/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/QVC5G48R/1404.html:text/html},
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	journaltitle = {{arXiv}:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	urldate = {2021-06-25},
	date = {2017-06-15},
	eprinttype = {arxiv},
	eprint = {1609.04747},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/WEKYH74S/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/6DBXKMZN/1609.html:text/html},
}

@article{kiwiel_convergence_2001,
	title = {Convergence and efficiency of subgradient methods for quasiconvex minimization},
	volume = {90},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/PL00011414},
	doi = {10.1007/PL00011414},
	abstract = {We study a general subgradient projection method for minimizing a quasiconvex objective subject to a convex set constraint in a Hilbert space. Our setting is very general: the objective is only upper semicontinuous on its domain, which need not be open, and various subdifferentials may be used. We extend previous results by proving convergence in objective values and to the generalized solution set for classical stepsizes tk→0, ∑tk=∞, and weak or strong convergence of the iterates to a solution for \{tk\}∈ℓ2∖ℓ1 under mild regularity conditions. For bounded constraint sets and suitable stepsizes, the method finds ε-solutions with an efficiency estimate of O(ε-2), thus being optimal in the sense of Nemirovskii.},
	pages = {1--25},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Kiwiel, Krzysztof C.},
	urldate = {2021-06-25},
	date = {2001-03-01},
	langid = {english},
}

@inproceedings{ciresan_flexible_2011,
	title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
	abstract = {We present a fast, fully parameterizable {GPU} implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification ({NORB}, {CIFAR}10) and handwritten digit recognition ({MNIST}), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. {NORB} is completely trained within five epochs. Test error rates on {MNIST} drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	eventtitle = {International Joint Conference on Artificial Intelligence {IJCAI}-2011},
	pages = {1237--1242},
	author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
	date = {2011-07-16},
	file = {Full Text PDF:/home/fabien/Zotero/storage/CPFPGNSM/Ciresan et al. - 2011 - Flexible, High Performance Convolutional Neural Ne.pdf:application/pdf},
}

@article{barkow_psychological_1992,
	title = {The Psychological Foundations of Culture},
	journaltitle = {The Adapted Mind: Evolutionary Psychology and the Generation of Culture,},
	shortjournal = {The Adapted Mind: Evolutionary Psychology and the Generation of Culture,},
	author = {Barkow, Jerome and Cosmides, Leda and {TOOBY}, {JOHN} and {WILLIAMS}, {GEORGE}},
	date = {1992-01-01},
	file = {Full Text PDF:/home/fabien/Zotero/storage/TMTQVL6A/Barkow et al. - 1992 - The Psychological Foundations of Culture.pdf:application/pdf},
}

@incollection{tooby_theoretical_2015,
	title = {The Theoretical Foundations of Evolutionary Psychology},
	abstract = {The human brain is the most highly organized system yet identified, and natural selection is the only physical process capable of pushing the designs of species uphill against entropy.Consequently, all functional mechanisms present in our species' neural architecture were constructed by selection acting on our ancestors.The design features of our psychological mechanisms are therefore linked directly as cause and effect to the specific structure of ancestral adaptive problems, allowing models of the adaptive problems identified by biologists and anthropologists to serve as maps that accelerate the discovery of previously unknown psychological mechanisms.The brain's function is specifically computational: to regulate behavior, development, and the body in ways that would have produced responses likely to have promoted genetic propagation under ancestral conditions.Evolutionary psychologists and allies are working toward two goals: The first is the progressive mapping of the program architectures (circuit logics) of the neurocomputational adaptations designed to solve ancestral adaptive problems. The second is the reformulation and theoretical unification of the social sciences made possible by an accurate, natural-science-based model of human nature.Here we sketch the discipline's theoretical foundations together with illustrative empirical discoveries (in reasoning, emotion and motivational programs, and parametric coordinative adaptations).},
	author = {Tooby, John and Cosmides, Leda},
	date = {2015-11-01},
	doi = {10.1002/9781119125563.evpsych101},
	file = {Full Text PDF:/home/fabien/Zotero/storage/NFJ2YADD/Tooby et Cosmides - 2015 - The Theoretical Foundations of Evolutionary Psycho.pdf:application/pdf},
}

@article{confer_evolutionary_2010,
	title = {Evolutionary Psychology Controversies, Questions, Prospects, and Limitations},
	volume = {65},
	doi = {10.1037/a0018413},
	abstract = {Evolutionary psychology has emerged over the past 15 years as a major theoretical perspective, generating an increasing volume of empirical studies and assuming a larger presence within psychological science. At the same time, it has generated critiques and remains controversial among some psychologists. Some of the controversy stems from hypotheses that go against traditional psychological theories; some from empirical findings that may have disturbing implications; some from misunderstandings about the logic of evolutionary psychology; and some from reasonable scientific concerns about its underlying framework. This article identifies some of the most common concerns and attempts to elucidate evolutionary psychology's stance pertaining to them. These include issues of testability and falsifiability; the domain specificity versus domain generality of psychological mechanisms; the role of novel environments as they interact with evolved psychological circuits; the role of genes in the conceptual structure of evolutionary psychology; the roles of learning, socialization, and culture in evolutionary psychology; and the practical value of applied evolutionary psychology. The article concludes with a discussion of the limitations of current evolutionary psychology.},
	pages = {110--26},
	journaltitle = {The American psychologist},
	shortjournal = {The American psychologist},
	author = {Confer, Jaime and Easton, Judith and Fleischman, Diana and Goetz, Cari and Lewis, David and Perilloux, Carin and Buss, David},
	date = {2010-02-01},
	file = {Full Text PDF:/home/fabien/Zotero/storage/GUJJL5LC/Confer et al. - 2010 - Evolutionary Psychology Controversies, Questions, .pdf:application/pdf},
}

@article{thomas_anterion_odd_2008,
	title = {An odd manifestation of the Capgras syndrome: loss of familiarity even with the sexual partner},
	volume = {38},
	issn = {0987-7053},
	doi = {10.1016/j.neucli.2008.04.003},
	shorttitle = {An odd manifestation of the Capgras syndrome},
	abstract = {We report the case of a patient who presented visual hallucinations and identification disorders associated with a Capgras syndrome. During the Capgras periods, there was not only a misidentification of his wife's face, but also a more global perceptive and emotional sexual identification disorder. Thus, he had sexual intercourse with his wife's "double" without having the slightest recollection feeling of familiarity towards his "wife" and even changed his sexual habits. To the best of our knowledge, he is the only neurological patient who made his wife a mistress. Starting from this global familiarity loss, we discuss the mechanism of Capgras delusion with reference to the role of the implicit system of face recognition. Such behavior of familiarity loss not only with face but also with all intimacy aspects argues for a specific disconnection between the ventral visual pathway of face identification and the limbic system involved in emotional and episodic memory contents.},
	pages = {177--182},
	number = {3},
	journaltitle = {Neurophysiologie Clinique = Clinical Neurophysiology},
	shortjournal = {Neurophysiol Clin},
	author = {Thomas Antérion, C. and Convers, P. and Desmales, S. and Borg, C. and Laurent, B.},
	date = {2008-06},
	pmid = {18539251},
	keywords = {Aged, Amnesia, Antipsychotic Agents, Atrophy, Brain, Capgras Syndrome, Donepezil, Hallucinations, Humans, Indans, Male, Memory, Movement Disorders, Neuropsychological Tests, Nootropic Agents, Parkinsonian Disorders, Piperidines, Recognition, Psychology, Risperidone, Sexual Behavior, Spouses, Tomography, Emission-Computed, Single-Photon, Tomography, X-Ray Computed},
}

@book{berntson_handbook_2009,
	title = {Handbook of Neuroscience for the Behavioral Sciences, Volume 1},
	isbn = {978-0-470-08356-7},
	abstract = {As technology has made imaging of the brain noninvasive and inexpensive, nearly every psychologist in every subfield is using pictures of the brain to show biological connections to feelings and behavior. Handbook of Neuroscience for the Behavioral Sciences, Volume I provides psychologists and other behavioral scientists with a solid foundation in the increasingly critical field of neuroscience. Current and accessible, this volume provides the information they need to understand the new biological bases, research tools, and implications of brain and gene research as it relates to psychology.},
	pagetotal = {722},
	publisher = {John Wiley \& Sons},
	author = {Berntson, Gary G. and Cacioppo, John T.},
	date = {2009-10-12},
	langid = {english},
	note = {Google-Books-{ID}: {LwdJhh}8bOvwC},
	keywords = {Psychology / General, Psychology / Neuropsychology},
}

@article{saleh_quantification_2014,
	title = {Quantification of cone loss after surgery for retinal detachment involving the macula using adaptive optics},
	volume = {98},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {0007-1161, 1468-2079},
	url = {https://bjo.bmj.com/content/98/10/1343},
	doi = {10.1136/bjophthalmol-2013-304813},
	abstract = {{\textless}h3{\textgreater}Aims{\textless}/h3{\textgreater} {\textless}p{\textgreater}To image the cones in eyes with anatomically successful repair of retinal detachment ({RD}) involving the macula and in healthy fellow eyes using an adaptive optics ({AO}) camera and to correlate the results to clinical outcomes.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater} {\textless}p{\textgreater}Twenty-one patients (42 eyes) operated for macula-off {RD} were imaged 6 weeks after surgery using an {AO} camera ({RTX} 1, Imagine Eyes, Orsay, France). Cone density (cells/mm$^{\textrm{2}}$), spacing between cells (µm) and the percentage of cones with six neighbours were measured. Best-corrected visual acuity ({BCVA}) and thickness of the inner segment ellipsoid ({ISe}) band imaged by {SD}-optical coherence tomography were also measured.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater} {\textless}p{\textgreater}The parafoveal cone density was decreased in eyes operated for {RD} (mean±{SD} 14 576±4035/mm$^{\textrm{2}}$) compared with fellow eyes (20 589±2350/mm$^{\textrm{2}}$) (p=0.0001). There was also an increase in cone spacing (10.3±2.6 vs 8.0±1.0.9 µm, respectively, p\&lt;0.0001). The nearest-neighbour analysis revealed a reduction in the percentage of cones with six neighbours (36.5±4.2 vs 42.7±4.6\%, p=0.0003). The {ISe} thickness, thinner in the operated eyes, was correlated to the cone density (r=0.62, p\&lt;0.0001). {BCVA} was significantly correlated to cone density (r=0.8, p\&lt;0.001).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions{\textless}/h3{\textgreater} {\textless}p{\textgreater}There was a decrease in the cone density after {RD} with an estimated loss of one-third of the cones. Postoperative visual acuity was highly correlated with the cone density. {AO} may be a valuable prognostic tool after {RD} surgery.{\textless}/p{\textgreater}},
	pages = {1343--1348},
	number = {10},
	journaltitle = {British Journal of Ophthalmology},
	author = {Saleh, M. and Debellemanière, G. and Meillat, M. and Tumahai, P. and Garnier, M. Bidaut and Flores, M. and Schwartz, C. and Delbosc, B.},
	urldate = {2021-06-25},
	date = {2014-10-01},
	langid = {english},
	pmid = {25237163},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: Clinical science},
	file = {Snapshot:/home/fabien/Zotero/storage/2PKP647Q/1343.html:text/html},
}

@article{purves_primary_2001,
	title = {The Primary Motor Cortex: Upper Motor Neurons That Initiate Complex Voluntary Movements},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK10962/},
	shorttitle = {The Primary Motor Cortex},
	abstract = {The upper motor neurons in the cerebral cortex reside in several adjacent and highly interconnected areas in the frontal lobe, which together mediate the planning and initiation of complex temporal sequences of voluntary movements. These cortical areas all receive regulatory input from the basal ganglia and cerebellum via relays in the ventrolateral thalamus (see Chapters 18 and 19), as well as inputs from the somatic sensory regions of the parietal lobe (see Chapter 9). Although the phrase “motor cortex” is sometimes used to refer to these frontal areas collectively, more commonly it is restricted to the primary motor cortex, which is located in the precentral gyrus (Figure 17.7). The primary motor cortex can be distinguished from the adjacent “premotor” areas both cytoarchitectonically (it is area 4 in Brodmann's nomenclature) and by the low intensity of current necessary to elicit movements by electrical stimulation in this region. The low threshold for eliciting movements is an indicator of a relatively large and direct pathway from the primary area to the lower motor neurons of the brainstem and spinal cord. This section and the next focus on the organization and functions of the primary motor cortex and its descending pathways, whereas the subsequent section addresses the contributions of the adjacent premotor areas.Figure 17.7The primary motor cortex and the premotor area in the human cerebral cortex as seen in lateral (A) and medial (B) views. The primary motor cortex is located in the precentral gyrus; the premotor area is more rostral.},
	journaltitle = {Neuroscience. 2nd edition},
	author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Katz, Lawrence C. and {LaMantia}, Anthony-Samuel and {McNamara}, James O. and Williams, S. Mark},
	urldate = {2021-06-25},
	date = {2001},
	langid = {english},
	note = {Publisher: Sinauer Associates},
	file = {Snapshot:/home/fabien/Zotero/storage/VXTHELPC/NBK10962.html:text/html},
}

@article{noauthor_ai_nodate,
	title = {The {AI} revolution in scientific research},
	pages = {10},
	langid = {english},
	file = {The AI revolution in scientific research.pdf:/home/fabien/Zotero/storage/JJH2FY6G/The AI revolution in scientific research.pdf:application/pdf},
}