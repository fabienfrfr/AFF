
@article{huxley_evolution_1942,
	title = {Evolution. The Modern Synthesis.},
	url = {https://www.cabdirect.org/cabdirect/abstract/19432202794},
	abstract = {T. H. {HUXLEY}, in an essay written 70 years ago, says "that whatever may be thought or said about Darwin's doctrines, or the manner in which he has propounded them, this much is certain, that, in a dozen years, the 'Origin of Species' has worked as complete a revolution in biological science as, the 'Principia' did in astronomy and it has done so because it contains an essentially new creative...},
	journaltitle = {Evolution. The Modern Synthesis.},
	author = {Huxley, J.},
	urldate = {2021-05-12},
	date = {1942},
	note = {Publisher: London: George Alien \& Unwin Ltd.},
	file = {Snapshot:/home/fabien/Zotero/storage/ST3JK2T6/19432202794.html:text/html},
}

@article{buick_when_2008,
	title = {When did oxygenic photosynthesis evolve?},
	volume = {363},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2008.0041},
	doi = {10.1098/rstb.2008.0041},
	abstract = {The atmosphere has apparently been oxygenated since the ‘Great Oxidation Event’ ca 2.4 Ga ago, but when the photosynthetic oxygen production began is debatable. However, geological and geochemical evidence from older sedimentary rocks indicates that oxygenic photosynthesis evolved well before this oxygenation event. Fluid-inclusion oils in ca 2.45 Ga sandstones contain hydrocarbon biomarkers evidently sourced from similarly ancient kerogen, preserved without subsequent contamination, and derived from organisms producing and requiring molecular oxygen. Mo and Re abundances and sulphur isotope systematics of slightly older (2.5 Ga) kerogenous shales record a transient pulse of atmospheric oxygen. As early as ca 2.7 Ga, stromatolites and biomarkers from evaporative lake sediments deficient in exogenous reducing power strongly imply that oxygen-producing cyanobacteria had already evolved. Even at ca 3.2 Ga, thick and widespread kerogenous shales are consistent with aerobic photoautrophic marine plankton, and U–Pb data from ca 3.8 Ga metasediments suggest that this metabolism could have arisen by the start of the geological record. Hence, the hypothesis that oxygenic photosynthesis evolved well before the atmosphere became permanently oxygenated seems well supported.},
	pages = {2731--2743},
	number = {1504},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Buick, Roger},
	urldate = {2021-05-12},
	date = {2008-08-27},
	note = {Publisher: Royal Society},
	file = {Texte intégral:/home/fabien/Zotero/storage/8VXK67BM/Buick - 2008 - When did oxygenic photosynthesis evolve.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/V2S2B7GJ/rstb.2008.html:text/html},
}

@article{mojzsis_evidence_1996,
	title = {Evidence for life on Earth before 3,800 million years ago},
	volume = {384},
	rights = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/384055a0},
	doi = {10.1038/384055a0},
	abstract = {{IT} is unknown when life first appeared on Earth. The earliest known microfossils (˜3,500 Myr before present) are structurally complex, and if it is assumed that the associated organisms required a long time to develop this degree of complexity, then the existence of life much earlier than this can be argued1,2. But the known examples of crustal rocks older than ˜3,500 Myr have experienced intense metamorphism, which would have obliterated any fragile microfossils contained therein. It is therefore necessary to search for geochemical evidence of past biotic activity that has been preserved within minerals that are resistant to metamorphism. Here we report ion-microprobe measure-ments of the carbon-isotope composition of carbonaceous inclusions within grains of apatite (basic calcium phosphate) from the oldest known sediment sequences—a ˜3,800-Myr-old banded iron formation from the Isua supracrustal belt, West Greenland35, and a similar formation from the nearby Akilia island that is possibly older than 3,850 Myr (ref. 3). The carbon in the carbonaceous inclusions is isotopically light, indicative of biological activity; no known abiotic process can explain the data. Unless some unknown abiotic process exists which is able both to create such isotopically light carbon and then selectively incorporate it into apatite grains, our results provide evidence for the emergence of life on Earth by at least 3,800 Myr before present.},
	pages = {55--59},
	number = {6604},
	journaltitle = {Nature},
	author = {Mojzsis, S. J. and Arrhenius, G. and {McKeegan}, K. D. and Harrison, T. M. and Nutman, A. P. and Friend, C. R. L.},
	urldate = {2021-05-12},
	date = {1996-11},
	langid = {english},
	note = {Number: 6604
Publisher: Nature Publishing Group},
	keywords = {Apatites, Carbon Isotopes, Carbonates, Earth, {NASA} Discipline Exobiology, Non-{NASA} Center, Planet, Time},
	file = {Snapshot:/home/fabien/Zotero/storage/9AITVTEV/384055a0.html:text/html},
}

@article{hedges_molecular_2004,
	title = {A molecular timescale of eukaryote evolution and the rise of complex multicellular life},
	volume = {4},
	issn = {1471-2148},
	url = {https://doi.org/10.1186/1471-2148-4-2},
	doi = {10.1186/1471-2148-4-2},
	abstract = {The pattern and timing of the rise in complex multicellular life during Earth's history has not been established. Great disparity persists between the pattern suggested by the fossil record and that estimated by molecular clocks, especially for plants, animals, fungi, and the deepest branches of the eukaryote tree. Here, we used all available protein sequence data and molecular clock methods to place constraints on the increase in complexity through time.},
	pages = {2},
	number = {1},
	journaltitle = {{BMC} Evolutionary Biology},
	shortjournal = {{BMC} Evolutionary Biology},
	author = {Hedges, S. Blair and Blair, Jaime E. and Venturi, Maria L. and Shoe, Jason L.},
	urldate = {2021-05-12},
	date = {2004-01-28},
	keywords = {Fossil Calibration, Molecular Clock, Primary Calibration, Protein Sequence Data, Secondary Calibration},
	file = {Full Text PDF:/home/fabien/Zotero/storage/EB6GHIQ5/Hedges et al. - 2004 - A molecular timescale of eukaryote evolution and t.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/U3I5E6EL/1471-2148-4-2.html:text/html},
}

@article{knoll_ediacaran_2006,
	title = {The Ediacaran Period: a new addition to the geologic time scale},
	volume = {39},
	issn = {1502-3931},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/00241160500409223},
	doi = {https://doi.org/10.1080/00241160500409223},
	shorttitle = {The Ediacaran Period},
	abstract = {The International Union of Geological Sciences has approved a new addition to the geologic time scale: the Ediacaran Period. The Ediacaran is the first Proterozoic period to be recognized on the basis of chronostratigraphic criteria and the first internationally ratified, chronostratigraphically defined period of any age to be introduced in more than a century. In accordance with procedures established by the International Commission on Stratigraphy, the base of the Ediacaran Period is defined by a Global Stratotype Section and Point ({GSSP}) placed at the base of the Nuccaleena Formation cap carbonate directly above glacial diamictites and associated facies at Enorama Creek in the Flinders Ranges of South Australia. Its top is defined by the initial {GSSP} of the Cambrian Period. The new Ediacaran Period encompasses a distinctive interval of Earth history that is bounded both above and below by equally distinctive intervals. Both chemostratigraphic and biostratigraphic data indicate that the subdivision of the period into two or more series is feasible, and this should be a primary objective of continuing work by the Ediacaran Subcommission of the {ICS}.},
	pages = {13--30},
	number = {1},
	journaltitle = {Lethaia},
	author = {Knoll, Andrew and Walter, Malcolm and Narbonne, Guy and Christie Blick, Nicholas},
	urldate = {2021-05-12},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/00241160500409223},
	keywords = {Ediacaran, Geologic time scale, global stratotype section and point, Proterozoic},
	file = {Snapshot:/home/fabien/Zotero/storage/8TZNNANI/00241160500409223.html:text/html;Version soumise:/home/fabien/Zotero/storage/4UGLV2K5/Knoll et al. - 2006 - The Ediacaran Period a new addition to the geolog.pdf:application/pdf},
}

@article{bonner_origins_1998,
	title = {The origins of multicellularity},
	volume = {1},
	rights = {Copyright © 1998 Wiley‐Liss, Inc.},
	issn = {1520-6602},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291520-6602%281998%291%3A1%3C27%3A%3AAID-INBI4%3E3.0.CO%3B2-6},
	doi = {https://doi.org/10.1002/(SICI)1520-6602(1998)1:1<27::AID-INBI4>3.0.CO;2-6},
	abstract = {There is great interest in the invention of multicellularity because it is one of the major transitions during the course of early evolution.1 Most of the emphasis has been on why it occurred. For instance, recently Gerhart and Kirschner2 have argued that a multicellular organism has gained the advantage of a unicellular ancestor because it can more effectively shield itself from the vagaries of the environment by producing its own internal environment. In broader terms, this is Dawkins'3 argument that a competitively effective way of carrying the genes from one generation to the next is by building a complex soma that safely sees to it that the germ plasm survives. © 1998 Wiley-Liss, Inc.},
	pages = {27--36},
	number = {1},
	journaltitle = {Integrative Biology: Issues, News, and Reviews},
	author = {Bonner, John Tyler},
	urldate = {2021-05-12},
	date = {1998},
	langid = {english},
	keywords = {cell differentiation, early evolution, multicellularity},
	file = {Snapshot:/home/fabien/Zotero/storage/NWRPCDSC/(SICI)1520-6602(1998)1127AID-INBI43.0.html:text/html},
}

@article{bartlett_defining_2020,
	title = {Defining Lyfe in the Universe: From Three Privileged Functions to Four Pillars},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2075-1729/10/4/42},
	doi = {10.3390/life10040042},
	shorttitle = {Defining Lyfe in the Universe},
	abstract = {Motivated by the need to paint a more general picture of what life is\&mdash;and could be\&mdash;with respect to the rest of the phenomena of the universe, we propose a new vocabulary for astrobiological research. Lyfe is defined as any system that fulfills all four processes of the living state, namely: dissipation, autocatalysis, homeostasis, and learning. Life is defined as the instance of lyfe that we are familiar with on Earth, one that uses a specific organometallic molecular toolbox to record information about its environment and achieve dynamical order by dissipating certain planetary disequilibria. This new classification system allows the astrobiological community to more clearly define the questions that propel their research\&mdash;e.g., whether they are developing a historical narrative to explain the origin of life (on Earth), or a universal narrative for the emergence of lyfe, or whether they are seeking signs of life specifically, or lyfe at large across the universe. While the concept of \&ldquo;life as we don\&rsquo;t know it\&rdquo; is not new, the four pillars of lyfe offer a novel perspective on the living state that is indifferent to the particular components that might produce it.},
	pages = {42},
	number = {4},
	journaltitle = {Life},
	author = {Bartlett, Stuart and Wong, Michael L.},
	urldate = {2021-05-12},
	date = {2020-04},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {astrobiology, definition of life, mechanotroph, origin of life},
	file = {Full Text PDF:/home/fabien/Zotero/storage/WY2QEZU9/Bartlett et Wong - 2020 - Defining Lyfe in the Universe From Three Privileg.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/K34UI4AT/42.html:text/html},
}

@article{maturanat_what_nodate,
	title = {What the Frog's Eye Tells the Frog's Brain},
	abstract = {In this paper, we analyze the activity of single fibers in the optic nerve of a frog. Our method is to find what sort of stimulus causes the largest activity in one nerve fiber and then what is the exciting aspect of that stimulus such that variations in everything else cause little change in the response. It has been known for the past 20 years that each fiber is connected not to a few rods and cones in the retina but to very many over a fair area. Our results show that for the most part within that area, it is not the light intensity itself but rather the pattern of local variation of intensity that is the exciting factor. There are four types of fibers, each type concerned with a different sort of pattern. Each type is uniformly distributed over the whole retina of the frog. Thus, there are four distinct parallel distributed channels whereby the frog's eye informs his brain about the visual image in terms of local pattern independent of average illumination. We describe the patterns and show the functional and anatomical separation of the channels. This work has been done on the frog, and our interpretation applies only to the frog.},
	pages = {12},
	journaltitle = {{PROCEEDINGS} {OF} {THE} {IRE}},
	author = {Maturanat, H R and Pitts, W H},
	langid = {english},
	file = {Maturanat et Pitts - What the Frog's Eye Tells the Frog's Brain.pdf:/home/fabien/Zotero/storage/K8KXTWRP/Maturanat et Pitts - What the Frog's Eye Tells the Frog's Brain.pdf:application/pdf},
}

@article{stuart_action_1997,
	title = {Action potential initiation and backpropagation in neurons of the mammalian {CNS}},
	volume = {20},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(96)10075-8},
	doi = {10.1016/S0166-2236(96)10075-8},
	pages = {125--131},
	number = {3},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Stuart, Greg and Spruston, Nelson and Sakmann, Bert and Häusser, Michael},
	urldate = {2021-05-14},
	date = {1997-03-01},
	pmid = {9061867},
	note = {Publisher: Elsevier},
	keywords = {active propogation, central nervous system, dendrites, neurons, sodium channels, synaptic integration},
	file = {Snapshot:/home/fabien/Zotero/storage/N6TCN3ZT/S0166-2236(96)10075-8.html:text/html;Snapshot:/home/fabien/Zotero/storage/PLP7WGBL/S0166-2236(96)10075-8.html:text/html},
}

@article{konda_actor-critic_nodate,
	title = {Actor-Critic Algorithms},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses {TD} learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	pages = {7},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	langid = {english},
	file = {Konda et Tsitsiklis - Actor-Critic Algorithms.pdf:/home/fabien/Zotero/storage/92TL3Z66/Konda et Tsitsiklis - Actor-Critic Algorithms.pdf:application/pdf;Full Text PDF:/home/fabien/Zotero/storage/D5VR3N7N/Konda et Gao - 2000 - Actor-critic algorithms.pdf:application/pdf},
}

@article{konda_onactor-critic_2003,
	title = {{OnActor}-Critic Algorithms},
	volume = {42},
	issn = {0363-0129, 1095-7138},
	url = {http://epubs.siam.org/doi/10.1137/S0363012901385691},
	doi = {10.1137/S0363012901385691},
	abstract = {In this article, we propose and analyze a class of actor-critic algorithms. These are two-time-scale algorithms in which the critic uses temporal diﬀerence learning with a linearly parameterized approximation architecture, and the actor is updated in an approximate gradient direction, based on information provided by the critic. We show that the features for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor. We study actor-critic algorithms for Markov decision processes with Polish state and action spaces. We state and prove two results regarding their convergence.},
	pages = {1143--1166},
	number = {4},
	journaltitle = {{SIAM} Journal on Control and Optimization},
	shortjournal = {{SIAM} J. Control Optim.},
	author = {Konda, Vijay R. and Tsitsiklis, John N.},
	urldate = {2021-05-14},
	date = {2003-01},
	langid = {english},
	file = {Konda et Tsitsiklis - 2003 - OnActor-Critic Algorithms.pdf:/home/fabien/Zotero/storage/DJGQSHKU/Konda et Tsitsiklis - 2003 - OnActor-Critic Algorithms.pdf:application/pdf},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: The third generation of neural network models},
	volume = {10},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	shorttitle = {Networks of spiking neurons},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on {McCulloch} Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	pages = {1659--1671},
	number = {9},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Maass, Wolfgang},
	urldate = {2021-06-23},
	date = {1997-12-01},
	langid = {english},
	keywords = {Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron},
	file = {ScienceDirect Snapshot:/home/fabien/Zotero/storage/254JDRY8/S0893608097000117.html:text/html},
}

@book{matthews_neurobiology_2000,
	title = {Neurobiology: Molecules, Cells and Systems},
	isbn = {978-0-632-04496-2},
	shorttitle = {Neurobiology},
	abstract = {Visit the Neurobiology Website at: www.blackwellpublishing.com/{matthewsAs} the second edition of a very successful neurobiology book, this text covers a range from molecules to systems, and uses various systems to illustrate each major concept. In addition to the text, this title offers a companion website, which features animations of difficult concepts, online assignments and practice exams, as well as all text figures in an easy to download format.Four colour throughout. New chapter on hypothalamic function with focus on circadian rhythms. More clinical correlation. Improved illustration quality and quantity. Comprehensive text with excellent coverage of subjects from molecules to systems. Use of systems to illustrate each major concept.},
	pagetotal = {604},
	publisher = {Wiley},
	author = {Matthews, Gary G.},
	date = {2000-12-27},
	langid = {english},
	keywords = {Medical / Neuroscience},
}

@book{anctil_dawn_2015,
	location = {Montreal ; Kingston ; London ; Chicago},
	title = {Dawn of the neuron: the early struggles to trace the origin of nervous systems},
	isbn = {978-0-7735-9732-7 978-0-7735-9733-4},
	shorttitle = {Dawn of the neuron},
	abstract = {"In science, sometimes it is best to keep things simple. Initially discrediting the discovery of neurons in jellyfish, mid-nineteenth-century scientists grouped jellyfish, comb-jellies, hydra, and sea anemones together under one term - "coelenterates"--And deemed these animals too similar to plants to warrant a nervous system. In Dawn of the Neuron, Michel Anctil shows how Darwin's theory of evolution completely eradicated this idea and cleared the way for the modern study of the neuron. Once zoologists accepted the notion that varying levels of animal complexity could evolve, they began to use simple-structured creatures such as coelenterates and sponges to understand the building blocks of more complicated nervous systems. Dawn of the Neuron provides fascinating insights into the labours and lives of scientists who studied coelenterate nervous systems over several generations, and who approached the puzzling origin of the first nerve cells through the process outlined in evolutionary theory. Anctil also reveals how these scientists, who were willing to embrace improved and paradigm-changing scientific methods, still revealed their cultural backgrounds, their societal biases, and their attachments to schools of thought and academic traditions while presenting their ground-breaking work. Their attitudes toward the neuron doctrine - where neurons are individual, self-contained cells - proved decisive in the exploration of how neurons first emerged. Featuring photographs and historical sketches to illustrate this quest for knowledge, Dawn of the Neuron is a remarkably in-depth exploration of the link between Darwin's theory of evolution and pioneering studies and understandings of the first evolved nervous systems, "--Amazon.com},
	pagetotal = {1},
	publisher = {{McGill}-Queen's University Press},
	author = {Anctil, Michel},
	date = {2015},
	note = {{OCLC}: 909956847},
	keywords = {Anatomy, Biological Evolution, Cnidaria, cytology, history, History, Life Sciences Human Anatomy \& Physiology, {MEDICAL}, Nervous system, Nervous System, Neuroanatomy, Neurons, Neurophysiology, {SCIENCE}},
}

@incollection{child_nervous_1921,
	location = {Chicago, {IL}, {US}},
	title = {Nervous centralization and cephalization in evolution},
	abstract = {The terms "centralization" and "central nervous system" possess of course a morphological and a physiological significance. Morphologically, centralization is the process, both ontogenetic and phylogenetic, of localization and aggregation of the chief mass of nervous tissue in a particular region of the body. The degree of integration corresponding to a particular degree of centralization or cephalization in a particular group depends upon the hereditary potentialities of the protoplasm concerned as regards the development of nervous structure and function. This chapter looks at centralization in relation to axiation, progressive centralization and cephalization in various groups, the "stepladder" type of nervous system in the invertebrates, and the position of peripheral receptors. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {142--154},
	booktitle = {The origin and development of the nervous system: From a physiological viewpoint},
	publisher = {University of Chicago Press},
	author = {Child, Charles Manning},
	date = {1921},
	doi = {10.1037/10916-009},
	keywords = {Central Nervous System, Morphology, Neural Development, Peripheral Nervous System, Physiology, Theory of Evolution},
	file = {Snapshot:/home/fabien/Zotero/storage/NBEI8C7X/2006-03519-009.html:text/html},
}

@article{chitwood_imitation_2014,
	title = {Imitation, Genetic Lineages, and Time Influenced the Morphological Evolution of the Violin},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0109229},
	doi = {10.1371/journal.pone.0109229},
	abstract = {Violin design has been in flux since the production of the first instruments in 16th century Italy. Numerous innovations have improved the acoustical properties and playability of violins. Yet, other attributes of the violin affect its performance less, and with fewer constraints, are potentially more sensitive to historical vagaries unrelated to quality. Although the coarse shape of violins is integral to their design, details of the body outline can vary without significantly compromising sound quality. What can violin shapes tell us about their makers and history, including the degree that luthiers have influenced each other and the evolution of complex morphologies over time? Here, I provide an analysis of morphological evolution in the violin family, sampling the body shapes of over 9,000 instruments over 400 years of history. Specific shape attributes, which discriminate instruments produced by different luthiers, strongly correlate with historical time. Linear discriminant analysis reveals luthiers who likely copied the outlines of their instruments from others, which historical accounts corroborate. Clustering of averaged violin shapes places luthiers into four major groups, demonstrating a handful of discrete shapes predominate in most instruments. Violin shapes originating from multi-generational luthier families tend to cluster together, and familial origin is a significant explanatory factor of violin shape. Together, the analysis of four centuries of violin shapes demonstrates not only the influence of history and time leading to the modern violin, but widespread imitation and the transmission of design by human relatedness.},
	pages = {e109229},
	number = {10},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Chitwood, Daniel H.},
	urldate = {2021-06-25},
	date = {2014-10-08},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Acoustics, Cities, Evolutionary genetics, Evolutionary processes, Fourier analysis, Hierarchical clustering, Linear discriminant analysis, Principal component analysis},
	file = {Full Text PDF:/home/fabien/Zotero/storage/UNLZAHH6/Chitwood - 2014 - Imitation, Genetic Lineages, and Time Influenced t.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/SHRDCSSL/article.html:text/html},
}

@article{rogers_natural_2008,
	title = {Natural selection and cultural rates of change},
	volume = {105},
	doi = {10.1073/pnas.0711802105},
	abstract = {It has been claimed that a meaningful theory of cultural evolution is not possible because human beliefs and behaviors do not follow predictable patterns. However, theoretical models of cultural transmission and observations of the development of societies suggest that patterns in cultural evolution do occur. Here, we analyze whether two sets of related cultural traits, one tested against the environment and the other not, evolve at different rates in the same populations. Using functional and symbolic design features for Polynesian canoes, we show that natural selection apparently slows the evolution of functional structures, whereas symbolic designs differentiate more rapidly. This finding indicates that cultural change, like genetic evolution, can follow theoretically derived patterns.

• canoe design
• cultural evolution
• Polynesia
• signatures of selection},
	pages = {3416--20},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rogers, Deborah and Ehrlich, Paul},
	date = {2008-04-01},
	file = {Texte intégral:/home/fabien/Zotero/storage/HD6YWMUR/Rogers et Ehrlich - 2008 - Natural selection and cultural rates of change.pdf:application/pdf},
}

@online{noauthor_nasa_nodate,
	title = {{NASA} - Life's Working Definition: Does It Work?},
	url = {https://www.nasa.gov/vision/universe/starsgalaxies/life%27s_working_definition.html},
	shorttitle = {{NASA} - Life's Working Definition},
	type = {Feature Articles},
	urldate = {2021-06-25},
	langid = {english},
	note = {Publisher: Brian Dunbar},
	file = {Snapshot:/home/fabien/Zotero/storage/L64BVA38/life's_working_definition.html:text/html},
}

@article{fredens_total_2019,
	title = {Total synthesis of Escherichia coli with a recoded genome},
	volume = {569},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7039709/},
	doi = {10.1038/s41586-019-1192-5},
	abstract = {Nature uses 64 codons to encode the synthesis of proteins from the genome, and chooses 1 sense codon—out of up to 6 synonyms—to encode each amino acid. Synonymous codon choice has diverse and important roles, and many synonymous substitutions are detrimental. Here we demonstrate that the number of codons used to encode the canonical amino acids can be reduced, through the genome-wide substitution of target codons by defined synonyms. We create a variant of Escherichia coli with a four-megabase synthetic genome through a high-fidelity convergent total synthesis. Our synthetic genome implements a defined recoding and refactoring scheme—with simple corrections at just seven positions—to replace every known occurrence of two sense codons and a stop codon in the genome. Thus, we recode 18,214 codons to create an organism with a 61-codon genome; this organism uses 59 codons to encode the 20 amino acids, and enables the deletion of a previously essential transfer {RNA}.},
	pages = {514--518},
	number = {7757},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Fredens, Julius and Wang, Kaihang and de la Torre, Daniel and Funke, Louise F. H. and Robertson, Wesley E. and Christova, Yonka and Chia, Tiongsun and Schmied, Wolfgang H. and Dunkelmann, Daniel and Beranek, Vaclav and Uttamapinant, Chayasith and Llamazares, Andres Gonzalez and Elliott, Thomas S. and Chin, Jason W.},
	urldate = {2021-06-25},
	date = {2019-05-01},
	pmid = {31092918},
	pmcid = {PMC7039709},
	file = {PubMed Central Full Text PDF:/home/fabien/Zotero/storage/DZBJ3X9J/Fredens et al. - 2019 - Total synthesis of Escherichia coli with a recoded.pdf:application/pdf},
}

@article{yaeger_computational_1995,
	title = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {PolyWorld}: Life in a New Context},
	shorttitle = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {PolyWorld}},
	abstract = {This paper discusses a computer model of living organisms and the ecology they exist in called {PolyWorld}. {PolyWorld} attempts to bring together all the principle components of real living systems into a single artificial (man-made) living system. {PolyWorld} brings together biologically motivated genetics, simple simulated physiologies and metabolisms, Hebbian learning in arbitrary neural network architectures, a visual perceptive mechanism, and a suite of primitive behaviors in artificial organisms grounded in an ecology just complex enough to foster speciation and inter-species competition. Predation, mimicry, sexual reproduction, and even communication are all supported in a straightforward fashion. The resulting survival strategies, both individual and group, are purely emergent, as are the functionalities embodied in their neural network "brains". Complex behaviors resulting from the simulated neural activity are unpredictable, and change as natural selection acts over multiple generations. In many ways, {PolyWorld} may be thought of as a sort of electronic primordial soup experiment, in the vein of Urey and Miller's [33] classic experiment, only commencing at a much higher level of organization. While one could claim that Urey and Miller really just threw a bunch of ingredients in a pot and watched to see what happened, the reason these men made a contribution to science rather than ratatouille is that they put the right ingredients in the right pot ... and watched to see what happened. Here we start with software-coded genetics and various simple nerve cells (lightsensitive, motor, and unspecified neuronal) as the ingredients, and place them in a competitive ecological crucible which subjects them to an internally consistent physics and the process of natural selectio...},
	author = {Yaeger, Larry},
	date = {1995-03-23},
	file = {Full Text PDF:/home/fabien/Zotero/storage/FII6GSWT/Yaeger - 1995 - Computational Genetics, Physiology, Metabolism, Ne.pdf:application/pdf},
}

@article{kemeny_theory_1967,
	title = {Theory of Self-Reproducing Automata. John von Neumann. Edited by Arthur W. Burks. University of Illinois Press, Urbana, 1966. 408 pp., illus. \$10},
	volume = {157},
	rights = {© 1967},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/157/3785/180.1},
	doi = {10.1126/science.157.3785.180},
	pages = {180--180},
	number = {3785},
	journaltitle = {Science},
	author = {Kemeny, John G.},
	urldate = {2021-06-25},
	date = {1967-07-14},
	langid = {english},
	note = {Publisher: American Association for the Advancement of Science
Section: Book Reviews},
	file = {Snapshot:/home/fabien/Zotero/storage/IZC5TNRY/180.html:text/html},
}

@article{noauthor_mathematical_1970,
	title = {Mathematical Games - The fantastic combinations of John Conway's new solitaire game "life" - M. Gardner - 1970},
	pages = {6},
	date = {1970},
	langid = {english},
	file = {1970 - Mathematical Games - The fantastic combinations of.pdf:/home/fabien/Zotero/storage/PW5S7NIM/1970 - Mathematical Games - The fantastic combinations of.pdf:application/pdf},
}

@incollection{wainwright_conways_2010,
	location = {London},
	title = {Conway’s Game of Life: Early Personal Recollections},
	isbn = {978-1-84996-217-9},
	url = {https://doi.org/10.1007/978-1-84996-217-9_2},
	shorttitle = {Conway’s Game of Life},
	abstract = {When the October 1970 issue of Scientific American arrived, I had no idea the extent to which Martin Gardner’s article in that issue would affect my life. As long as I can remember, my custom would be to seek out the Mathematical Games column in search for Gardner’s latest topic with the usual reader challenges. My first reaction to that particular article introducing a new pastime titled “The fantastic combinations of John Conway’s new solitaire game ‘life''' was only mildly interesting. A couple of days later, still curious about the outcome of random patterns, I located an old checkerboard and a small jarful of pennies to investigate this new game.},
	pages = {11--16},
	booktitle = {Game of Life Cellular Automata},
	publisher = {Springer},
	author = {Wainwright, Robert},
	editor = {Adamatzky, Andrew},
	urldate = {2021-06-25},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-1-84996-217-9_2},
}

@book{berlekamp_winning_2003,
	location = {New York},
	edition = {2},
	title = {Winning Ways for Your Mathematical Plays},
	isbn = {978-0-429-48732-3},
	abstract = {In the quarter of a century since three mathematicians and game theorists collaborated to create Winning Ways for Your Mathematical Plays, the book has become the definitive work on the subject of mathematical games. Now carefully revised and broken down into four volumes to accommodate new developments, the Second Edition retains the original's wealth of wit and wisdom. The authors' insightful strategies, blended with their witty and irreverent style, make reading a profitable pleasure. In Volume 2, the authors have a Change of Heart, bending the rules established in Volume 1 to apply them to games such as Cut-cake and Loopy Hackenbush. From the Table of Contents: - If You Can't Beat 'Em, Join 'Em! - Hot Bottles Followed by Cold Wars - Games Infinite and Indefinite - Games Eternal--Games Entailed - Survival in the Lost World},
	pagetotal = {212},
	publisher = {A K Peters/{CRC} Press},
	author = {Berlekamp, Elwyn R. and Conway, John H. and Guy, Richard K.},
	date = {2003-03-01},
	doi = {10.1201/9780429487323},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urldate = {2021-06-25},
	date = {1943-12-01},
	langid = {english},
}

@article{frs_liii_1901,
	title = {{LIII}. On lines and planes of closest fit to systems of points in space},
	volume = {2},
	issn = {1941-5982},
	url = {https://doi.org/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	pages = {559--572},
	number = {11},
	journaltitle = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {F.R.S, Karl Pearson},
	urldate = {2021-06-25},
	date = {1901-11-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14786440109462720},
	file = {Version soumise:/home/fabien/Zotero/storage/VJUUTJ8B/F.R.S - 1901 - LIII. On lines and planes of closest fit to system.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/T8PED4KG/14786440109462720.html:text/html},
}

@book{vapnik_nature_2000,
	location = {New York},
	edition = {2},
	title = {The Nature of Statistical Learning Theory},
	isbn = {978-0-387-98780-4},
	url = {https://www.springer.com/gp/book/9780387987804},
	series = {Information Science and Statistics},
	abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and {SVM} techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader {AT}\&T Labs-Research and Professor of London University. He is one of the founders of},
	publisher = {Springer-Verlag},
	author = {Vapnik, Vladimir},
	urldate = {2021-06-25},
	date = {2000},
	langid = {english},
	doi = {10.1007/978-1-4757-3264-1},
	file = {Snapshot:/home/fabien/Zotero/storage/4MVQQ8QR/9780387987804.html:text/html},
}

@incollection{bock_clustering_2007,
	location = {Berlin, Heidelberg},
	title = {Clustering Methods: A History of k-Means Algorithms},
	isbn = {978-3-540-73560-1},
	url = {https://doi.org/10.1007/978-3-540-73560-1_15},
	series = {Studies in Classification, Data Analysis, and Knowledge Organization},
	shorttitle = {Clustering Methods},
	abstract = {This paper surveys some historical issues related to the well-known k-means algorithm in cluster analysis. It shows to which authors the different versions of this algorithm can be traced back, and which were the underlying applications. We sketch various generalizations (with references also to Diday’s work) and thereby underline the usefulness of the k-means approach in data analysis.},
	pages = {161--172},
	booktitle = {Selected Contributions in Data Analysis and Classification},
	publisher = {Springer},
	author = {Bock, Hans-Hermann},
	editor = {Brito, Paula and Cucumel, Guy and Bertrand, Patrice and de Carvalho, Francisco},
	urldate = {2021-06-25},
	date = {2007},
	langid = {english},
	doi = {10.1007/978-3-540-73560-1_15},
	keywords = {Class Centroid, Class Prototype, Cluster Criterion, Comptes Rendus Acad, Supply Point},
}

@article{hinton_learning_2007,
	title = {Learning multiple layers of representation},
	volume = {11},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661307002173},
	doi = {10.1016/j.tics.2007.09.004},
	abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
	pages = {428--434},
	number = {10},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Hinton, Geoffrey E.},
	urldate = {2021-06-25},
	date = {2007-10-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/fabien/Zotero/storage/VJEH9TYZ/S1364661307002173.html:text/html},
}

@article{morris_do_1999,
	title = {D.O. Hebb: The Organization of Behavior, Wiley: New York; 1949},
	volume = {50},
	issn = {0361-9230},
	doi = {10.1016/s0361-9230(99)00182-3},
	shorttitle = {D.O. Hebb},
	pages = {437},
	number = {5},
	journaltitle = {Brain Research Bulletin},
	shortjournal = {Brain Res Bull},
	author = {Morris, R. G.},
	date = {1999-12},
	pmid = {10643472},
	keywords = {History, Animals, Behavior, Animal, Cognitive Science, History, 20th Century, Neurosciences, Publishing, Behavior, 20th Century, Animal},
}

@book{minsky_perceptrons_1988,
	location = {Cambridge, Mass},
	edition = {Expanded ed},
	title = {Perceptrons: an introduction to computational geometry},
	isbn = {978-0-262-63111-2},
	shorttitle = {Perceptrons},
	pagetotal = {292},
	publisher = {{MIT} Press},
	author = {Minsky, Marvin and Papert, Seymour},
	date = {1988},
	keywords = {Data processing, Geometry, Machine learning, Parallel processing (Electronic computers), Perceptrons},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2021-06-25},
	date = {1986-10},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6088
Primary\_atype: Research
Publisher: Nature Publishing Group},
	file = {Snapshot:/home/fabien/Zotero/storage/VAI6WC3F/323533a0.html:text/html},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {0022-3751},
	doi = {10.1113/jphysiol.1968.sp008455},
	abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers ({II} and the upper two-thirds of {III}) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer {III}, and in {IV} A and {IV} B. In layer {IV} B they form a large proportion of the population, whereas complex cells are rare. In layers {IV} A and {IV} B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer {IV} most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and {VI} contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer {IV}, the higher orders in the upper and lower layers.},
	pages = {215--243},
	number = {1},
	journaltitle = {The Journal of Physiology},
	shortjournal = {J Physiol},
	author = {Hubel, D. H. and Wiesel, T. N.},
	date = {1968-03},
	pmid = {4966457},
	pmcid = {PMC1557912},
	keywords = {Animals, Color Perception, Evoked Potentials, Haplorhini, Light, Motion Perception, Occipital Lobe, Retina, Vision, Ocular, Visual Fields, Ocular, Vision},
	file = {Texte intégral:/home/fabien/Zotero/storage/VAZ74Q99/Hubel et Wiesel - 1968 - Receptive fields and functional architecture of mo.pdf:application/pdf},
}

@article{ciresan_multi-column_2012,
	title = {Multi-column Deep Neural Networks for Image Classification},
	url = {http://arxiv.org/abs/1202.2745},
	abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive {MNIST} handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	journaltitle = {{arXiv}:1202.2745 [cs]},
	author = {Ciresan, Dan and Meier, Ueli and Schmidhuber, Juergen},
	urldate = {2021-06-25},
	date = {2012-02-13},
	eprinttype = {arxiv},
	eprint = {1202.2745},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/836NDNCH/Cireşan et al. - 2012 - Multi-column Deep Neural Networks for Image Classi.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/QFTD3ZQH/1202.html:text/html},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	pages = {279--292},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	urldate = {2021-06-25},
	date = {1992-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/fabien/Zotero/storage/AFU92BXF/Watkins et Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2021-06-25},
	date = {1997-11-15},
	file = {Full Text PDF:/home/fabien/Zotero/storage/HFGIXR7J/Hochreiter et Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/7YGE9IDJ/Long-Short-Term-Memory.html:text/html},
}

@article{stanley_evolving_2002,
	title = {Evolving Neural Networks through Augmenting Topologies},
	volume = {10},
	issn = {1063-6560},
	doi = {10.1162/106365602320169811},
	abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, {NeuroEvolution} of Augmenting Topologies ({NEAT}), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. {NEAT} is also an important contribution to {GAs} because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
	pages = {99--127},
	number = {2},
	journaltitle = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	date = {2002-06},
	note = {Conference Name: Evolutionary Computation},
	keywords = {competing conventions, Genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	file = {IEEE Xplore Abstract Record:/home/fabien/Zotero/storage/LJV6QYRF/6790655.html:text/html},
}

@article{rakison_infants_2008,
	title = {Do infants possess an evolved spider-detection mechanism?},
	volume = {107},
	issn = {1873-7838(Electronic),0010-0277(Print)},
	doi = {10.1016/j.cognition.2007.07.022},
	abstract = {Previous studies with various non-human animals have revealed that they possess an evolved predator recognition mechanism that specifies the appearance of recurring threats. We used the preferential looking and habituation paradigms in three experiments to investigate whether 5-month-old human infants have a perceptual template for spiders that generalizes to real-world images of spiders. A fourth experiment assessed whether 5-month-olds have a perceptual template for a non-threatening biological stimulus (i.e., a flower). The results supported the hypothesis that humans, like other species, may possess a cognitive mechanism for detecting specific animals that were potentially harmful throughout evolutionary history. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {381--393},
	number = {1},
	journaltitle = {Cognition},
	author = {Rakison, David H. and Derringer, Jaime},
	date = {2008},
	note = {Place: Netherlands
Publisher: Elsevier Science},
	keywords = {Arachnida, Cognitive Development, Infant Development, Recognition (Learning)},
	file = {Snapshot:/home/fabien/Zotero/storage/MHR6J8QM/2008-03366-020.html:text/html},
}

@article{rasch_about_2013,
	title = {About Sleep's Role in Memory},
	volume = {93},
	issn = {0031-9333},
	url = {https://journals.physiology.org/doi/full/10.1152/physrev.00032.2012},
	doi = {10.1152/physrev.00032.2012},
	abstract = {Over more than a century of research has established the fact that sleep benefits the retention of memory. In this review we aim to comprehensively cover the field of “sleep and memory” research by providing a historical perspective on concepts and a discussion of more recent key findings. Whereas initial theories posed a passive role for sleep enhancing memories by protecting them from interfering stimuli, current theories highlight an active role for sleep in which memories undergo a process of system consolidation during sleep. Whereas older research concentrated on the role of rapid-eye-movement ({REM}) sleep, recent work has revealed the importance of slow-wave sleep ({SWS}) for memory consolidation and also enlightened some of the underlying electrophysiological, neurochemical, and genetic mechanisms, as well as developmental aspects in these processes. Specifically, newer findings characterize sleep as a brain state optimizing memory consolidation, in opposition to the waking brain being optimized for encoding of memories. Consolidation originates from reactivation of recently encoded neuronal memory representations, which occur during {SWS} and transform respective representations for integration into long-term memory. Ensuing {REM} sleep may stabilize transformed memories. While elaborated with respect to hippocampus-dependent memories, the concept of an active redistribution of memory representations from networks serving as temporary store into long-term stores might hold also for non-hippocampus-dependent memory, and even for nonneuronal, i.e., immunological memories, giving rise to the idea that the offline consolidation of memory during sleep represents a principle of long-term memory formation established in quite different physiological systems.},
	pages = {681--766},
	number = {2},
	journaltitle = {Physiological Reviews},
	author = {Rasch, Björn and Born, Jan},
	urldate = {2021-06-25},
	date = {2013-04-01},
	note = {Publisher: American Physiological Society},
	file = {Full Text PDF:/home/fabien/Zotero/storage/TH4YLPP4/Rasch et Born - 2013 - About Sleep's Role in Memory.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/25PPFUEM/physrev.00032.html:text/html},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in Neural Networks: An Overview},
	volume = {61},
	issn = {08936080},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	shorttitle = {Deep Learning in Neural Networks},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	urldate = {2021-06-25},
	date = {2015-01},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/7AJAD3VY/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/QVC5G48R/1404.html:text/html},
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	journaltitle = {{arXiv}:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	urldate = {2021-06-25},
	date = {2017-06-15},
	eprinttype = {arxiv},
	eprint = {1609.04747},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/WEKYH74S/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/6DBXKMZN/1609.html:text/html},
}

@article{kiwiel_convergence_2001,
	title = {Convergence and efficiency of subgradient methods for quasiconvex minimization},
	volume = {90},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/PL00011414},
	doi = {10.1007/PL00011414},
	abstract = {We study a general subgradient projection method for minimizing a quasiconvex objective subject to a convex set constraint in a Hilbert space. Our setting is very general: the objective is only upper semicontinuous on its domain, which need not be open, and various subdifferentials may be used. We extend previous results by proving convergence in objective values and to the generalized solution set for classical stepsizes tk→0, ∑tk=∞, and weak or strong convergence of the iterates to a solution for \{tk\}∈ℓ2∖ℓ1 under mild regularity conditions. For bounded constraint sets and suitable stepsizes, the method finds ε-solutions with an efficiency estimate of O(ε-2), thus being optimal in the sense of Nemirovskii.},
	pages = {1--25},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Kiwiel, Krzysztof C.},
	urldate = {2021-06-25},
	date = {2001-03-01},
	langid = {english},
}

@inproceedings{ciresan_flexible_2011,
	title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
	abstract = {We present a fast, fully parameterizable {GPU} implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification ({NORB}, {CIFAR}10) and handwritten digit recognition ({MNIST}), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. {NORB} is completely trained within five epochs. Test error rates on {MNIST} drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	eventtitle = {International Joint Conference on Artificial Intelligence {IJCAI}-2011},
	pages = {1237--1242},
	author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
	date = {2011-07-16},
	file = {Full Text PDF:/home/fabien/Zotero/storage/CPFPGNSM/Ciresan et al. - 2011 - Flexible, High Performance Convolutional Neural Ne.pdf:application/pdf},
}

@article{barkow_psychological_1992,
	title = {The Psychological Foundations of Culture},
	journaltitle = {The Adapted Mind: Evolutionary Psychology and the Generation of Culture,},
	shortjournal = {The Adapted Mind: Evolutionary Psychology and the Generation of Culture,},
	author = {Barkow, Jerome and Cosmides, Leda and {TOOBY}, {JOHN} and {WILLIAMS}, {GEORGE}},
	date = {1992-01-01},
	file = {Full Text PDF:/home/fabien/Zotero/storage/TMTQVL6A/Barkow et al. - 1992 - The Psychological Foundations of Culture.pdf:application/pdf},
}

@incollection{tooby_theoretical_2015,
	title = {The Theoretical Foundations of Evolutionary Psychology},
	abstract = {The human brain is the most highly organized system yet identified, and natural selection is the only physical process capable of pushing the designs of species uphill against entropy.Consequently, all functional mechanisms present in our species' neural architecture were constructed by selection acting on our ancestors.The design features of our psychological mechanisms are therefore linked directly as cause and effect to the specific structure of ancestral adaptive problems, allowing models of the adaptive problems identified by biologists and anthropologists to serve as maps that accelerate the discovery of previously unknown psychological mechanisms.The brain's function is specifically computational: to regulate behavior, development, and the body in ways that would have produced responses likely to have promoted genetic propagation under ancestral conditions.Evolutionary psychologists and allies are working toward two goals: The first is the progressive mapping of the program architectures (circuit logics) of the neurocomputational adaptations designed to solve ancestral adaptive problems. The second is the reformulation and theoretical unification of the social sciences made possible by an accurate, natural-science-based model of human nature.Here we sketch the discipline's theoretical foundations together with illustrative empirical discoveries (in reasoning, emotion and motivational programs, and parametric coordinative adaptations).},
	author = {Tooby, John and Cosmides, Leda},
	date = {2015-11-01},
	doi = {10.1002/9781119125563.evpsych101},
	file = {Full Text PDF:/home/fabien/Zotero/storage/NFJ2YADD/Tooby et Cosmides - 2015 - The Theoretical Foundations of Evolutionary Psycho.pdf:application/pdf},
}

@article{confer_evolutionary_2010,
	title = {Evolutionary Psychology Controversies, Questions, Prospects, and Limitations},
	volume = {65},
	doi = {10.1037/a0018413},
	abstract = {Evolutionary psychology has emerged over the past 15 years as a major theoretical perspective, generating an increasing volume of empirical studies and assuming a larger presence within psychological science. At the same time, it has generated critiques and remains controversial among some psychologists. Some of the controversy stems from hypotheses that go against traditional psychological theories; some from empirical findings that may have disturbing implications; some from misunderstandings about the logic of evolutionary psychology; and some from reasonable scientific concerns about its underlying framework. This article identifies some of the most common concerns and attempts to elucidate evolutionary psychology's stance pertaining to them. These include issues of testability and falsifiability; the domain specificity versus domain generality of psychological mechanisms; the role of novel environments as they interact with evolved psychological circuits; the role of genes in the conceptual structure of evolutionary psychology; the roles of learning, socialization, and culture in evolutionary psychology; and the practical value of applied evolutionary psychology. The article concludes with a discussion of the limitations of current evolutionary psychology.},
	pages = {110--26},
	journaltitle = {The American psychologist},
	shortjournal = {The American psychologist},
	author = {Confer, Jaime and Easton, Judith and Fleischman, Diana and Goetz, Cari and Lewis, David and Perilloux, Carin and Buss, David},
	date = {2010-02-01},
	file = {Full Text PDF:/home/fabien/Zotero/storage/GUJJL5LC/Confer et al. - 2010 - Evolutionary Psychology Controversies, Questions, .pdf:application/pdf},
}

@article{thomas_anterion_odd_2008,
	title = {An odd manifestation of the Capgras syndrome: loss of familiarity even with the sexual partner},
	volume = {38},
	issn = {0987-7053},
	doi = {10.1016/j.neucli.2008.04.003},
	shorttitle = {An odd manifestation of the Capgras syndrome},
	abstract = {We report the case of a patient who presented visual hallucinations and identification disorders associated with a Capgras syndrome. During the Capgras periods, there was not only a misidentification of his wife's face, but also a more global perceptive and emotional sexual identification disorder. Thus, he had sexual intercourse with his wife's "double" without having the slightest recollection feeling of familiarity towards his "wife" and even changed his sexual habits. To the best of our knowledge, he is the only neurological patient who made his wife a mistress. Starting from this global familiarity loss, we discuss the mechanism of Capgras delusion with reference to the role of the implicit system of face recognition. Such behavior of familiarity loss not only with face but also with all intimacy aspects argues for a specific disconnection between the ventral visual pathway of face identification and the limbic system involved in emotional and episodic memory contents.},
	pages = {177--182},
	number = {3},
	journaltitle = {Neurophysiologie Clinique = Clinical Neurophysiology},
	shortjournal = {Neurophysiol Clin},
	author = {Thomas Anterion, C. and Convers, P. and Desmales, S. and Borg, C. and Laurent, B.},
	date = {2008-06},
	pmid = {18539251},
	keywords = {Aged, Amnesia, Antipsychotic Agents, Atrophy, Brain, Capgras Syndrome, Donepezil, Hallucinations, Humans, Indans, Male, Memory, Movement Disorders, Neuropsychological Tests, Nootropic Agents, Parkinsonian Disorders, Piperidines, Recognition, Psychology, Risperidone, Sexual Behavior, Spouses, Tomography, Emission-Computed, Single-Photon, Tomography, X-Ray Computed, Emission-Computed, Psychology, Recognition, Single-Photon, Tomography, X-Ray Computed},
}

@book{berntson_handbook_2009,
	title = {Handbook of Neuroscience for the Behavioral Sciences, Volume 1},
	isbn = {978-0-470-08356-7},
	abstract = {As technology has made imaging of the brain noninvasive and inexpensive, nearly every psychologist in every subfield is using pictures of the brain to show biological connections to feelings and behavior. Handbook of Neuroscience for the Behavioral Sciences, Volume I provides psychologists and other behavioral scientists with a solid foundation in the increasingly critical field of neuroscience. Current and accessible, this volume provides the information they need to understand the new biological bases, research tools, and implications of brain and gene research as it relates to psychology.},
	pagetotal = {722},
	publisher = {John Wiley \& Sons},
	author = {Berntson, Gary G. and Cacioppo, John T.},
	date = {2009-10-12},
	langid = {english},
	note = {Google-Books-{ID}: {LwdJhh}8bOvwC},
	keywords = {Psychology / General, Psychology / Neuropsychology},
}

@article{saleh_quantification_2014,
	title = {Quantification of cone loss after surgery for retinal detachment involving the macula using adaptive optics},
	volume = {98},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {0007-1161, 1468-2079},
	url = {https://bjo.bmj.com/content/98/10/1343},
	doi = {10.1136/bjophthalmol-2013-304813},
	abstract = {{\textless}h3{\textgreater}Aims{\textless}/h3{\textgreater} {\textless}p{\textgreater}To image the cones in eyes with anatomically successful repair of retinal detachment ({RD}) involving the macula and in healthy fellow eyes using an adaptive optics ({AO}) camera and to correlate the results to clinical outcomes.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater} {\textless}p{\textgreater}Twenty-one patients (42 eyes) operated for macula-off {RD} were imaged 6 weeks after surgery using an {AO} camera ({RTX} 1, Imagine Eyes, Orsay, France). Cone density (cells/mm$^{\textrm{2}}$), spacing between cells (µm) and the percentage of cones with six neighbours were measured. Best-corrected visual acuity ({BCVA}) and thickness of the inner segment ellipsoid ({ISe}) band imaged by {SD}-optical coherence tomography were also measured.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater} {\textless}p{\textgreater}The parafoveal cone density was decreased in eyes operated for {RD} (mean±{SD} 14 576±4035/mm$^{\textrm{2}}$) compared with fellow eyes (20 589±2350/mm$^{\textrm{2}}$) (p=0.0001). There was also an increase in cone spacing (10.3±2.6 vs 8.0±1.0.9 µm, respectively, p\&lt;0.0001). The nearest-neighbour analysis revealed a reduction in the percentage of cones with six neighbours (36.5±4.2 vs 42.7±4.6\%, p=0.0003). The {ISe} thickness, thinner in the operated eyes, was correlated to the cone density (r=0.62, p\&lt;0.0001). {BCVA} was significantly correlated to cone density (r=0.8, p\&lt;0.001).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions{\textless}/h3{\textgreater} {\textless}p{\textgreater}There was a decrease in the cone density after {RD} with an estimated loss of one-third of the cones. Postoperative visual acuity was highly correlated with the cone density. {AO} may be a valuable prognostic tool after {RD} surgery.{\textless}/p{\textgreater}},
	pages = {1343--1348},
	number = {10},
	journaltitle = {British Journal of Ophthalmology},
	author = {Saleh, M. and Debellemanière, G. and Meillat, M. and Tumahai, P. and Garnier, M. Bidaut and Flores, M. and Schwartz, C. and Delbosc, B.},
	urldate = {2021-06-25},
	date = {2014-10-01},
	langid = {english},
	pmid = {25237163},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: Clinical science},
	file = {Snapshot:/home/fabien/Zotero/storage/2PKP647Q/1343.html:text/html},
}

@article{purves_primary_2001,
	title = {The Primary Motor Cortex: Upper Motor Neurons That Initiate Complex Voluntary Movements},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK10962/},
	shorttitle = {The Primary Motor Cortex},
	abstract = {The upper motor neurons in the cerebral cortex reside in several adjacent and highly interconnected areas in the frontal lobe, which together mediate the planning and initiation of complex temporal sequences of voluntary movements. These cortical areas all receive regulatory input from the basal ganglia and cerebellum via relays in the ventrolateral thalamus (see Chapters 18 and 19), as well as inputs from the somatic sensory regions of the parietal lobe (see Chapter 9). Although the phrase “motor cortex” is sometimes used to refer to these frontal areas collectively, more commonly it is restricted to the primary motor cortex, which is located in the precentral gyrus (Figure 17.7). The primary motor cortex can be distinguished from the adjacent “premotor” areas both cytoarchitectonically (it is area 4 in Brodmann's nomenclature) and by the low intensity of current necessary to elicit movements by electrical stimulation in this region. The low threshold for eliciting movements is an indicator of a relatively large and direct pathway from the primary area to the lower motor neurons of the brainstem and spinal cord. This section and the next focus on the organization and functions of the primary motor cortex and its descending pathways, whereas the subsequent section addresses the contributions of the adjacent premotor areas.Figure 17.7The primary motor cortex and the premotor area in the human cerebral cortex as seen in lateral (A) and medial (B) views. The primary motor cortex is located in the precentral gyrus; the premotor area is more rostral.},
	journaltitle = {Neuroscience. 2nd edition},
	author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Katz, Lawrence C. and {LaMantia}, Anthony-Samuel and {McNamara}, James O. and Williams, S. Mark},
	urldate = {2021-06-25},
	date = {2001},
	langid = {english},
	note = {Publisher: Sinauer Associates},
	file = {Snapshot:/home/fabien/Zotero/storage/VXTHELPC/NBK10962.html:text/html},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	journaltitle = {{arXiv}:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2021-06-28},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/5TZ6KC6V/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/S5PQJIM9/2005.html:text/html},
}

@book{mowrer_learning_1960,
	location = {Hoboken, {NJ}, {US}},
	title = {Learning theory and behavior},
	series = {Learning theory and behavior},
	abstract = {The aim of this book is to achieve a high level of synthesis regarding learning theory and behavior. The author attempts to do so by examining both research and conjecture in a broadly historical context, in addition to presenting new experimental findings not available to earlier system makers and theorists. In this way, it is believed, empirical facts and divergent theories become maximally meaningful and most significantly related. The book begins with an introductory chapter that presents a historical review and perspective of the field of learning theory. Chapter 2 examines the law of effect, conditioning, and punishment. Chapter 3 discusses two versions of two-factor learning theory. In the fourth chapter, two conceptions of secondary reinforcement are presented. Chapters 5 and 6 continue the examination of secondary reinforcement with discussions of a unifying theory and reservations and complications. The topics of Chapter 7 are a revised two-factor theory and the concept of habit, followed by Chapter 8 which comparatively examines other theories and some further evidence. Hope, fear, and field theory are the focus of Chapter 9, and Chapter 10 focuses on reinforcement gradients and temporal integration. The book closes with two chapters on unlearning, conflict, frustration, courage, generalization, discrimination, and skill. The basic argument proposed by the author is epitomized in Chapter 7. Earlier chapters provide the logical and factual background from which this argument evolves; and the five subsequent chapters amplify and apply the argument in more specific ways. Thus, the reader who wishes a quick "look" at this volume as a whole may first read the chapter indicated; but the argument will unfold most naturally and persuasively if the chapters are read in the order in which they appear. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pagetotal = {xii, 555},
	publisher = {John Wiley \& Sons Inc},
	author = {Mowrer, O. Hobart},
	date = {1960},
	doi = {10.1037/10802-000},
	note = {Pages: xii, 555},
	keywords = {Behavior, Conditioning, Discrimination Learning, Generalization (Learning), Habits, History of Psychology, Learning Theory, Punishment, Secondary Reinforcement},
	file = {Snapshot:/home/fabien/Zotero/storage/H9RKC4K4/2005-06665-000.html:text/html},
}

@book{pavlov_lectures_1928,
	location = {New York, {NY}, {US}},
	title = {Lectures on conditioned reflexes: Twenty-five years of objective study of the higher nervous activity (behaviour) of animals},
	series = {Lectures on conditioned reflexes: Twenty-five years of objective study of the higher nervous activity (behaviour) of animals},
	shorttitle = {Lectures on conditioned reflexes},
	abstract = {Over a quarter of a century ago Pavlov began, "after persistent deliberation," to study the activity of the higher parts of the brain by physiological methods, mainly through the use of his conditioned reflexes. This book shows the historical development of the subject from the very beginning to the present, and it contains most of the important facts which he has discovered. As the subject is a new and somewhat difficult one, the many repetitions in varying form will probably not be unwelcome to the reader. Because the facts and methods described in the following pages cover most of Pavlov's important work on conditioned reflexes, it seemed worth while to us to make the translation from the original Russian. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pagetotal = {414},
	publisher = {Liverwright Publishing Corporation},
	author = {Pavlov, Ivan Petrovitch},
	editorb = {Gantt, W. Horsley},
	editorbtype = {redactor},
	date = {1928},
	doi = {10.1037/11081-000},
	note = {Pages: 414},
	keywords = {Neurophysiology, History of Psychology, Classical Conditioning, Conditioned Responses, Experimentation, Physiological Psychology, Theories},
	file = {Snapshot:/home/fabien/Zotero/storage/B65U6424/2006-07574-000.html:text/html},
}

@incollection{wainwright_conways_2010-1,
	location = {London},
	title = {Conway’s Game of Life: Early Personal Recollections},
	isbn = {978-1-84996-217-9},
	url = {https://doi.org/10.1007/978-1-84996-217-9_2},
	shorttitle = {Conway’s Game of Life},
	abstract = {When the October 1970 issue of Scientific American arrived, I had no idea the extent to which Martin Gardner’s article in that issue would affect my life. As long as I can remember, my custom would be to seek out the Mathematical Games column in search for Gardner’s latest topic with the usual reader challenges. My first reaction to that particular article introducing a new pastime titled “The fantastic combinations of John Conway’s new solitaire game ‘life‴ was only mildly interesting. A couple of days later, still curious about the outcome of random patterns, I located an old checkerboard and a small jarful of pennies to investigate this new game.},
	pages = {11--16},
	booktitle = {Game of Life Cellular Automata},
	publisher = {Springer},
	author = {Wainwright, Robert},
	editor = {Adamatzky, Andrew},
	urldate = {2021-06-25},
	date = {2010},
	doi = {10.1007/978-1-84996-217-9_2},
}

@article{al-jarrah_efficient_2015,
	title = {Efficient Machine Learning for Big Data: A Review},
	volume = {2},
	issn = {2214-5796},
	url = {https://www.sciencedirect.com/science/article/pii/S2214579615000271},
	doi = {10.1016/j.bdr.2015.04.001},
	series = {Big Data, Analytics, and High-Performance Computing},
	shorttitle = {Efficient Machine Learning for Big Data},
	abstract = {With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years – in fact, as much as 90\% of current data were created in the last couple of years – a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven – the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas' structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.},
	pages = {87--93},
	number = {3},
	journaltitle = {Big Data Research},
	shortjournal = {Big Data Research},
	author = {Al-Jarrah, Omar Y. and Yoo, Paul D. and Muhaidat, Sami and Karagiannidis, George K. and Taha, Kamal},
	urldate = {2021-06-27},
	date = {2015-09-01},
	keywords = {Big data, Computational modeling, Efficient machine learning, Green computing},
}

@inproceedings{malinowski_minimal_1994,
	title = {Minimal training set size estimation for neural network-based function approximation},
	volume = {6},
	doi = {10.1109/ISCAS.1994.409611},
	abstract = {A new approach to the problem of n-dimensional continuous and sampled-data function approximation using a two-layer neural network is presented. The generalized Nyquist theorem is introduced to solve for the optimum number of training examples in n-dimensional input space. Choosing the smallest but still sufficient set of training vectors results in a reduced learning time for the network. Analytical formulas and algorithm for training set size reduction are developed and illustrated by two-dimensional data examples.{\textbackslash}textless{\textbackslash}textgreater},
	eventtitle = {Proceedings of {IEEE} International Symposium on Circuits and Systems - {ISCAS} '94},
	pages = {403--406 vol.6},
	booktitle = {Proceedings of {IEEE} International Symposium on Circuits and Systems - {ISCAS} '94},
	author = {Malinowski, A. and Zurada, J.R. and Aronhime, P.B.},
	date = {1994-05},
	keywords = {Electronic mail, Fourier transforms, Frequency estimation, Function approximation, Multi-layer neural network, Multidimensional systems, Neural networks, Sampling methods, Signal restoration, Training data},
}

@incollection{modrell_primitive_2021,
	location = {Treasure Island ({FL})},
	title = {Primitive Reflexes},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK554606/},
	abstract = {Primitive reflexes are involuntary motor responses originating in the brainstem present after birth in early child development that facilitate survival. Several reflexes are important in the assessment of newborns and young infants. These central nervous system motor responses are eventually inhibited by 4 to 6 months of age as the brain matures and replaces them with voluntary motor activities but may return with the presence of neurological disease.[1][2]},
	booktitle = {{StatPearls}},
	publisher = {{StatPearls} Publishing},
	author = {Modrell, Alexa K. and Tadi, Prasanna},
	urldate = {2021-06-27},
	date = {2021},
	pmid = {32119493},
}

@article{garwicz_unifying_2009,
	title = {A unifying model for timing of walking onset in humans and other mammals},
	volume = {106},
	pages = {21889--21893},
	number = {51},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Garwicz, Martin and Christensson, Maria and Psouni, Elia},
	date = {2009-12-22},
}

@article{deng_mnist_2012,
	title = {The {MNIST} Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},
	volume = {29},
	issn = {1558-0792},
	doi = {10.1109/MSP.2012.2211477},
	abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology ({MNIST}) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
	pages = {141--142},
	number = {6},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Deng, Li},
	date = {2012-11},
	note = {Conference Name: {IEEE} Signal Processing Magazine},
	keywords = {Machine learning},
	file = {IEEE Xplore Abstract Record:/home/fabien/Zotero/storage/66Z74QBD/6296535.html:text/html},
}

@article{markov_example_2006,
	title = {An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains},
	doi = {10.1017/S0269889706001074},
	abstract = {This study investigates a text excerpt containing 20,000 Russian letters of the alphabet, excluding \${\textbackslash}Cprime\$ and \${\textbackslash}Cdprime\$, from Pushkin's novel Eugene Onegin–the entire first chapter and sixteen stanzas of the second.},
	journaltitle = {Science in Context},
	author = {Markov, A.},
	date = {2006},
}

@report{bellman_theory_1954,
	title = {{THE} {THEORY} {OF} {DYNAMIC} {PROGRAMMING}},
	url = {https://apps.dtic.mil/sti/citations/AD0604386},
	abstract = {The paper is the text of an invited address before the annual summer meeting of the American Mathematical Society at Laramie, Wyoming, September 2, 1954. The contents are chiefly of an expository nature on the theory of dynamic programming.},
	institution = {{RAND} {CORP} {SANTA} {MONICA} {CA}},
	author = {Bellman, Richard},
	urldate = {2021-06-28},
	date = {1954-07-30},
	langid = {english},
	note = {Section: Technical Reports},
	file = {Full Text PDF:/home/fabien/Zotero/storage/XX3GDQSM/Bellman - 1954 - THE THEORY OF DYNAMIC PROGRAMMING.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/F59G6GFB/AD0604386.html:text/html},
}

@article{auer_finite-time_2002,
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	volume = {47},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1013689704352},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	pages = {235--256},
	number = {2},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
	urldate = {2021-06-28},
	date = {2002-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/fabien/Zotero/storage/IEFJD4TN/Auer et al. - 2002 - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf},
}

@article{metropolis_monte_1949,
	title = {The Monte Carlo Method},
	volume = {44},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2280232},
	doi = {10.2307/2280232},
	abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	pages = {335--341},
	number = {247},
	journaltitle = {Journal of the American Statistical Association},
	author = {Metropolis, Nicholas and Ulam, S.},
	urldate = {2021-06-28},
	date = {1949},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
}

@article{abeywardena_application_1972,
	title = {An application of principal component analysis in genetics},
	volume = {61},
	issn = {0022-1333},
	url = {https://doi.org/10.1007/BF02984099},
	doi = {10.1007/BF02984099},
	abstract = {A statistical experiment designed to test the efficiency of different methods of estimatingrepeatability is described.},
	pages = {27--51},
	number = {1},
	journaltitle = {Journal of Genetics},
	shortjournal = {J Genet},
	author = {Abeywardena, V.},
	urldate = {2021-06-28},
	date = {1972-07-01},
	langid = {english},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: A review of methods and applications},
	volume = {1},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	shorttitle = {Graph neural networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks ({GNNs}) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of {GNNs} such as graph convolutional network ({GCN}), graph attention network ({GAT}), graph recurrent network ({GRN}) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for {GNN} models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	pages = {57--81},
	journaltitle = {{AI} Open},
	shortjournal = {{AI} Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	urldate = {2021-06-28},
	date = {2020-01-01},
	langid = {english},
	keywords = {Deep learning, Graph neural network},
	file = {ScienceDirect Full Text PDF:/home/fabien/Zotero/storage/8ISXH75H/Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf:application/pdf;ScienceDirect Snapshot:/home/fabien/Zotero/storage/EWG3S32L/S2666651021000012.html:text/html},
}

@inproceedings{joachimczak_fine_2014,
	title = {Fine Grained Artificial Development for Body-Controller Coevolution of Soft-Bodied Animats},
	isbn = {978-0-262-32621-6},
	doi = {10.7551/978-0-262-32621-6-ch040},
	pages = {239--246},
	author = {Joachimczak, Michał and Suzuki, Reiji and Arita, Takaya},
	date = {2014-07-30},
	file = {Full Text PDF:/home/fabien/Zotero/storage/HHMB4EH5/Joachimczak et al. - 2014 - Fine Grained Artificial Development for Body-Contr.pdf:application/pdf},
}

@article{orr_fitness_2009,
	title = {Fitness and its role in evolutionary genetics},
	volume = {10},
	rights = {2009 Nature Publishing Group},
	issn = {1471-0064},
	url = {https://www.nature.com/articles/nrg2603},
	doi = {10.1038/nrg2603},
	abstract = {This article starts by reviewing the differences between various measures of fitness, for example, individual fitness, absolute fitness, relative fitness and geometric mean fitness.Differences in fitness (when measured appropriately) can be used to derive selection equations, which show how natural selection changes the genetic composition of a population through time.Quantitative geneticists derived the secondary theorem of natural selection, which shows how selection on fitness will change other, genetically correlated traits.Fitness landscape models, whether continuous or discrete, can be used to analyse how natural selection will drive a population to the top of a fitness peak.Evolutionary geneticists are currently pursuing several empirical approaches to the study of fitness, including direct fitness assays, microbial experimental evolution and the use of {DNA} sequence data to infer a history of positive natural selection.The concluding section sketches several major unresolved problems in the experimental study of fitness.},
	pages = {531--539},
	number = {8},
	journaltitle = {Nature Reviews Genetics},
	shortjournal = {Nat Rev Genet},
	author = {Orr, H. Allen},
	urldate = {2021-06-29},
	date = {2009-08},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	file = {Version acceptée:/home/fabien/Zotero/storage/FGEEGBGG/Orr - 2009 - Fitness and its role in evolutionary genetics.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/8VIIVF7Y/nrg2603.html:text/html},
}

@article{schelling_dynamic_1971,
	title = {Dynamic models of segregation},
	volume = {1},
	issn = {0022-250X},
	url = {https://doi.org/10.1080/0022250X.1971.9989794},
	doi = {10.1080/0022250X.1971.9989794},
	abstract = {Some segregation results from the practices of organizations, some from specialized communication systems, some from correlation with a variable that is non‐random; and some results from the interplay of individual choices. This is an abstract study of the interactive dynamics of discriminatory individual choices. One model is a simulation in which individual members of two recognizable groups distribute themselves in neighborhoods defined by reference to their own locations. A second model is analytic and deals with compartmented space. A final section applies the analytics to ‘neighborhood tipping.’ The systemic effects are found to be overwhelming: there is no simple correspondence of individual incentive to collective results. Exaggerated separation and patterning result from the dynamics of movement. Inferences about individual motives can usually not be drawn from aggregate patterns. Some unexpected phenomena, like density and vacancy, are generated. A general theory of ‘tipping’ begins to emerge.},
	pages = {143--186},
	number = {2},
	journaltitle = {The Journal of Mathematical Sociology},
	author = {Schelling, Thomas C.},
	urldate = {2021-06-29},
	date = {1971-07-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/0022250X.1971.9989794},
	file = {Snapshot:/home/fabien/Zotero/storage/RIQL543L/0022250X.1971.html:text/html},
}

@article{jones_attribution_1967,
	title = {The attribution of attitudes},
	volume = {3},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/0022103167900340},
	doi = {10.1016/0022-1031(67)90034-0},
	abstract = {Three experiments were conducted within the framework of correspondent inference theory. In each of the experiments the subjects were instructed to estimate the “true” attitude of a target person after having either read or listened to a speech by him expressing opinions on a controversial topic. Independent variables included position of speech (pro, anti, or equivocal), choice of position vs. assignment of position, and reference group of target person. The major hypothesis (which was confirmed with varying strength in all three experiments) was that choice would make a greater difference when there was a low prior probability of someone taking the position expressed in the speech. Other findings of interest were: (1) a tendency to attribute attitude in line with behavior, even in no-choice conditions; (2) increased inter-individual variability in conditions where low probability opinions were expressed in a constraining context; (3) that this variability was partly a function of the subjects' own attitudes on the issue; (4) that equivocation in no-choice conditions leads to the attribution that the equivocator opposes the assigned position. The main conclusion suggested is that perceivers do take account of prior probabilities and situational constraints when attributing private attitude, but perhaps do not weight these factors as heavily as would be expected by a rational analysis.},
	pages = {1--24},
	number = {1},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Jones, Edward E and Harris, Victor A},
	urldate = {2021-06-29},
	date = {1967-01-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/fabien/Zotero/storage/94KINZ42/0022103167900340.html:text/html},
}

@article{vicsek_novel_1995,
	title = {Novel type of phase transition in a system of self-driven particles},
	volume = {75},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/cond-mat/0611743},
	doi = {10.1103/PhysRevLett.75.1226},
	abstract = {A simple model with a novel type of dynamics is introduced in order to investigate the emergence of self-ordered motion in systems of particles with biologically motivated interaction. In our model particles are driven with a constant absolute velocity and at each time step assume the average direction of motion of the particles in their neighborhood with some random perturbation (\${\textbackslash}eta\$) added. We present numerical evidence that this model results in a kinetic phase transition from no transport (zero average velocity, \${\textbar} \{{\textbackslash}bf v\}\_a {\textbar} =0\$) to finite net transport through spontaneous symmetry breaking of the rotational symmetry. The transition is continuous since \${\textbar} \{{\textbackslash}bf v\}\_a {\textbar}\$ is found to scale as \$({\textbackslash}eta\_c-{\textbackslash}eta){\textasciicircum}{\textbackslash}beta\$ with \${\textbackslash}beta{\textbackslash}simeq 0.45\$.},
	pages = {1226--1229},
	number = {6},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Vicsek, Tamas and Czirok, Andras and Ben-Jacob, Eshel and Cohen, Inon and Sochet, Ofer},
	urldate = {2021-06-29},
	date = {1995-08-07},
	eprinttype = {arxiv},
	eprint = {cond-mat/0611743},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/MNFV5FLU/Vicsek et al. - 1995 - Novel type of phase transition in a system of self.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/T6KNYD8Z/0611743.html:text/html},
}

@article{press_iterated_2012,
	title = {Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent},
	volume = {109},
	pages = {10409--10413},
	number = {26},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Press, William H. and Dyson, Freeman J.},
	date = {2012-06-26},
	file = {Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent | PNAS:/home/fabien/Zotero/storage/I5LKBDAW/10409.html:text/html},
}

@article{stewart_extortion_2012,
	title = {Extortion and cooperation in the Prisoner’s Dilemma},
	volume = {109},
	pages = {10134--10135},
	number = {26},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Stewart, Alexander J. and Plotkin, Joshua B.},
	date = {2012-06-26},
	file = {Extortion and cooperation in the Prisoner’s Dilemma | PNAS:/home/fabien/Zotero/storage/MUZMKAV4/10134.html:text/html},
}

@report{wei_civilization_2012,
	location = {Rochester, {NY}},
	title = {'Civilization' and 'Culture'},
	url = {https://papers.ssrn.com/abstract=2182608},
	abstract = {As semantic entanglement exists between ‘civilization’ and ‘culture’ in civilizational studies, almost everyone feels free or even obliged to make up his own definition, with the consequence of serious confusion. This article discusses the historical usages of ‘civilization’ and ‘culture’ in the 18th and 19th centuries and the various definitions advanced by important thinkers such as Oswald Spengler, Fernand Braudel and Philip Bagby and puts forward its own way of dealing with the terms. It advances the views that the emphasis of ‘civilization’ is often on a historico-cultural entity or congeries of peoples sharing a common geographic locus, common values and social institutions, rather than on a particular set of values or beliefs itself, and that the distinction between ‘civilization’ as the largest and highest socio-historical unit and ‘culture’ as something smaller, lower and subsumed under ‘civilization’ is productive for a meaningful explanation of the ubiquitous phenomenon of cultural appropriation and civilizational hybridization. It believes that any attempt at a brief, accurate yet cognitively meaningful definition of the two terms does not help; rather, a seemingly clumsy way of giving a detailed depiction might be constructive for disentangling the entanglement, clarifying the concepts, thus promoting civilizational studies.},
	number = {{ID} 2182608},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Wei, Ruan},
	urldate = {2021-06-29},
	date = {2012-11-29},
	langid = {english},
	keywords = {Bagby, Braudel, Civilization, culture, globalization, religion},
	file = {Snapshot:/home/fabien/Zotero/storage/2DVUUZHH/papers.html:text/html},
}

@book{marshall_prisoners_2016,
	title = {Prisoners of Geography: Ten Maps That Explain Everything About the World},
	isbn = {978-1-5011-2147-0},
	shorttitle = {Prisoners of Geography},
	abstract = {In this New York Times bestseller, an award-winning journalist uses ten maps of crucial regions to explain the geo-political strategies of the world powers—“fans of geography, history, and politics (and maps) will be enthralled” (Fort Worth Star-Telegram).Maps have a mysterious hold over us. Whether ancient, crumbling parchments or generated by Google, maps tell us things we want to know, not only about our current location or where we are going but about the world in general. And yet, when it comes to geo-politics, much of what we are told is generated by analysts and other experts who have neglected to refer to a map of the place in question. All leaders of nations are constrained by geography. In “one of the best books about geopolitics” (The Evening Standard), now updated to include 2016 geopolitical developments, journalist Tim Marshall examines Russia, China, the {US}, Latin America, the Middle East, Africa, Europe, Japan, Korea, and Greenland and the Arctic—their weather, seas, mountains, rivers, deserts, and borders—to provide a context often missing from our political reportage: how the physical characteristics of these countries affect their strengths and vulnerabilities and the decisions made by their leaders. Offering “a fresh way of looking at maps” (The New York Times Book Review), Marshall explains the complex geo-political strategies that shape the globe. Why is Putin so obsessed with Crimea? Why was the {US} destined to become a global superpower? Why does China’s power base continue to expand? Why is Tibet destined to lose its autonomy? Why will Europe never be united? The answers are geographical. “In an ever more complex, chaotic, and interlinked world, Prisoners of Geography is a concise and useful primer on geopolitics” (Newsweek) and a critical guide to one of the major determining factors in world affairs.},
	pagetotal = {320},
	publisher = {Simon and Schuster},
	author = {Marshall, Tim},
	date = {2016-10-11},
	langid = {english},
	note = {Google-Books-{ID}: 46cxDQAAQBAJ},
	keywords = {History / Historical Geography, History / World, Political Science / Geopolitics, Social Science / Human Geography},
}

@book{bourdieu_distinction_1984,
	title = {Distinction : a social critique of the judgement of taste},
	isbn = {978-0-674-21277-0},
	url = {http://archive.org/details/distinctionsocia0000bour},
	shorttitle = {Distinction},
	abstract = {xiv, 613 p. : 25 cm; Translation of: La distinction : critique sociale du jugement; Includes bibliographical references and index; 96 09 16},
	pagetotal = {640},
	publisher = {Cambridge, {MA}. : Harvard University Press},
	author = {Bourdieu, Pierre},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2021-06-29},
	date = {1984},
	keywords = {Aesthetics, French},
}

@book{henaff_claude_1998,
	title = {Claude Levi-Strauss and the Making of Structural Anthropology},
	isbn = {978-0-8166-2761-5},
	abstract = {The most comprehensive, precise, and up-to-date account of the work of Claude Levi-Strauss yet written (and Levi-Strauss's favorite presentation of his own work), this book offers an unparalleled view of the thought of the man who single-handedly reinvented anthropology. With close attention to the wide range of knowledge that informs Levi-Strauss's work, Marcel Henaff has written an authoritative and accessible analysis of Levi-Strauss's research in anthropological theory and practice as well as his contributions to the debates surrounding linguistics, epistemology, ethics, and aesthetics.The book begins with a clear explication of the concept of structure, both demonstrating the relevance and defining the limits of the structural approach to the social sciences. Henaff's focus, like that of his subject, is social anthropology. Within this field, he gives painstaking attention to three areas that distinguish Levi-Strauss's work: kinship systems, systems of classification, and mythologies. His discussion moves from broad questions such as "What differentiates 'historical' societies from those called primitive?" to particular ones, such as "What does Levi-Strauss mean by symbolism? totemism? wild thought?"From these topics, Henaff goes on to consider general philosophical issues concerning the theoretical status of anthropology and the cultural status of the anthropologist. In light of Levi-Strauss's work, he looks at ideas about the universality of the human mind and logical categories, the representation of time in oral cultures and their relation to history, and the responsibility of Western societies to now-vanishing oral cultures. An extensive chronology and detailedbibliography present a valuable overview of Levi-Strauss's career.As debates surrounding structuralism subside, and as anthropology continues to transform itself, this book at last affords a broad and balanced account of the remarkable accomplishments of one of the great intellectual innovators of the twentieth century.},
	pagetotal = {312},
	publisher = {U of Minnesota Press},
	author = {Henaff, Marcel},
	date = {1998},
	langid = {english},
	keywords = {Social Science / Anthropology / General},
}

@article{cheatham_molecular_1995,
	title = {Molecular Dynamics Simulations on Solvated Biomolecular Systems: The Particle Mesh Ewald Method Leads to Stable Trajectories of {DNA}, {RNA}, and Proteins},
	volume = {117},
	issn = {0002-7863},
	url = {https://doi.org/10.1021/ja00119a045},
	doi = {10.1021/ja00119a045},
	shorttitle = {Molecular Dynamics Simulations on Solvated Biomolecular Systems},
	pages = {4193--4194},
	number = {14},
	journaltitle = {Journal of the American Chemical Society},
	shortjournal = {J. Am. Chem. Soc.},
	author = {Cheatham, T. E. {III} and Miller, J. L. and Fox, T. and Darden, T. A. and Kollman, P. A.},
	urldate = {2021-06-29},
	date = {1995-04-01},
	note = {Publisher: American Chemical Society},
	file = {ACS Full Text Snapshot:/home/fabien/Zotero/storage/VFVNRRHT/ja00119a045.html:text/html},
}

@article{fowler_evidence_2013,
	title = {Evidence of Evolution},
	url = {https://opentextbc.ca/conceptsofbiologyopenstax/chapter/evidence-of-evolution/},
	author = {Fowler, Samantha and Roush, Rebecca and Wise, James},
	urldate = {2021-06-29},
	date = {2013-04-25},
	langid = {english},
	note = {Book Title: Concepts of Biology
Publisher: {OpenStax}},
	file = {Snapshot:/home/fabien/Zotero/storage/6266MDYW/evidence-of-evolution.html:text/html},
}

@article{leijnen_neural_2020,
	title = {The Neural Network Zoo},
	volume = {47},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2504-3900/47/1/9},
	doi = {10.3390/proceedings2020047009},
	abstract = {An overview of neural network architectures is presented. Some of these architectures have been created in recent years, whereas others originate from many decades ago. Apart from providing a practical tool for comparing deep learning models, the Neural Network Zoo also uncovers a taxonomy of network architectures, their chronology, and traces back lineages and inspirations for these neural information processing systems.},
	pages = {9},
	number = {1},
	journaltitle = {Proceedings},
	author = {Leijnen, Stefan and Veen, Fjodor van},
	urldate = {2021-06-29},
	date = {2020},
	langid = {english},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {neural networks, artificial intelligence, connectionism, deep learning, neural information processing, neural network architectures},
	file = {Full Text PDF:/home/fabien/Zotero/storage/AYZ5W49B/Leijnen et Veen - 2020 - The Neural Network Zoo.pdf:application/pdf;Snapshot:/home/fabien/Zotero/storage/RNKI8N5U/9.html:text/html},
}

@inproceedings{bhowmick_louvainne_2020,
	location = {Houston, United States},
	title = {{LouvainNE}: Hierarchical Louvain Method for High Quality and Scalable Network Embedding *},
	url = {https://hal.archives-ouvertes.fr/hal-02999888},
	shorttitle = {{LouvainNE}},
	abstract = {Network embedding, that aims to learn low-dimensional vector representation of nodes such that the network structure is preserved, has gained significant research attention in recent years. However, most state-of-the-art network embedding methods are computa-tionally expensive and hence unsuitable for representing nodes in billion-scale networks. In this paper, we present {LouvainNE}, a hierarchical clustering approach to network embedding. Precisely, we employ Louvain, an extremely fast and accurate community detection method, to build a hierarchy of successively smaller subgraphs. We obtain representations of individual nodes in the original graph at different levels of the hierarchy, then we aggregate these representations to learn the final embedding vectors. Our theoretical analysis shows that our proposed algorithm has quasi-linear run-time and memory complexity. Our extensive experimental evaluation , carried out on multiple real-world networks of different scales, demonstrates both (i) the scalability of our proposed approach that can handle graphs containing tens of billions of edges, as well as (ii) its effectiveness in performing downstream network mining tasks such as network reconstruction and node classification.},
	booktitle = {the 13th {ACM} International {WSDM} Conference},
	author = {Bhowmick, Ayan Kumar and Meneni, Koushik and Danisch, Maximilien and Guillaume, Jean-Loup and Mitra, Bivas},
	urldate = {2021-06-29},
	date = {2020},
	keywords = {\${\textbackslash}bullet\$ Mathematics of computing \${\textbackslash}rightarrow\$ Graph algorithms {KEYWORDS} Network embedding, {CCS} {CONCEPTS} \${\textbackslash}bullet\$ Computing methodologies \${\textbackslash}rightarrow\$ Machine learning algorithms, real-world graph algorithms, scalability},
	file = {HAL PDF Full Text:/home/fabien/Zotero/storage/A8QT4QAC/Bhowmick et al. - 2020 - LouvainNE Hierarchical Louvain Method for High Qu.pdf:application/pdf},
}

@article{liu_learning_2017,
	title = {Learning Affinity via Spatial Propagation Networks},
	url = {http://arxiv.org/abs/1710.01020},
	abstract = {In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep {CNN}, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier. We validate the framework on the task of refinement for image segmentation boundaries. Experiments on the {HELEN} face parsing and {PASCAL} {VOC}-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.},
	journaltitle = {{arXiv}:1710.01020 [cs]},
	author = {Liu, Sifei and De Mello, Shalini and Gu, Jinwei and Zhong, Guangyu and Yang, Ming-Hsuan and Kautz, Jan},
	urldate = {2021-06-29},
	date = {2017-10-03},
	eprinttype = {arxiv},
	eprint = {1710.01020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/J7G6FB5Q/Liu et al. - 2017 - Learning Affinity via Spatial Propagation Networks.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/MSACEIKV/1710.html:text/html},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut learning in deep neural networks},
	volume = {2},
	rights = {2020 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00257-z},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	pages = {665--673},
	number = {11},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	urldate = {2021-06-29},
	date = {2020-11},
	langid = {english},
	file = {Snapshot:/home/fabien/Zotero/storage/E5F5PING/s42256-020-00257-z.html:text/html;Version soumise:/home/fabien/Zotero/storage/LDNN6ES7/Geirhos et al. - 2020 - Shortcut learning in deep neural networks.pdf:application/pdf},
}

@article{brockman_openai_2016,
	title = {{OpenAI} Gym},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {{OpenAI} Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
	journaltitle = {{arXiv}:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	urldate = {2021-06-29},
	date = {2016-06-05},
	eprinttype = {arxiv},
	eprint = {1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/4PGHCLHS/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/NA5XMBWR/1606.html:text/html},
}

@article{godin_dual_2018,
	title = {Dual Rectified Linear Units ({DReLUs}): A Replacement for Tanh Activation Functions in Quasi-Recurrent Neural Networks},
	volume = {116},
	issn = {01678655},
	url = {http://arxiv.org/abs/1707.08214},
	doi = {10.1016/j.patrec.2018.09.006},
	shorttitle = {Dual Rectified Linear Units ({DReLUs})},
	abstract = {In this paper, we introduce a novel type of Rectified Linear Unit ({ReLU}), called a Dual Rectified Linear Unit ({DReLU}). A {DReLU}, which comes with an unbounded positive and negative image, can be used as a drop-in replacement for a tanh activation function in the recurrent step of Quasi-Recurrent Neural Networks ({QRNNs}) (Bradbury et al. (2017)). Similar to {ReLUs}, {DReLUs} are less prone to the vanishing gradient problem, they are noise robust, and they induce sparse activations. We independently reproduce the {QRNN} experiments of Bradbury et al. (2017) and compare our {DReLU}-based {QRNNs} with the original tanh-based {QRNNs} and Long Short-Term Memory networks ({LSTMs}) on sentiment classification and word-level language modeling. Additionally, we evaluate on character-level language modeling, showing that we are able to stack up to eight {QRNN} layers with {DReLUs}, thus making it possible to improve the current state-of-the-art in character-level language modeling over shallow architectures based on {LSTMs}.},
	pages = {8--14},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Godin, Fréderic and Degrave, Jonas and Dambre, Joni and De Neve, Wesley},
	urldate = {2021-06-29},
	date = {2018-12},
	eprinttype = {arxiv},
	eprint = {1707.08214},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/SA93LQGB/Godin et al. - 2018 - Dual Rectified Linear Units (DReLUs) A Replacemen.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/CDVBNELN/1707.html:text/html},
}

@article{smith_developmental_1985,
	title = {Developmental Constraints and Evolution: A Perspective from the Mountain Lake Conference on Development and Evolution},
	volume = {60},
	issn = {0033-5770},
	url = {https://www.journals.uchicago.edu/doi/10.1086/414425},
	doi = {10.1086/414425},
	shorttitle = {Developmental Constraints and Evolution},
	abstract = {Developmental constraints (defined as biases on the production of variant phenotypes or limitations on phenotypic variability caused by the structure, character, composition, or dynamics of the developmental system) undoubtedly play a significant role in evolution. Yet there is little agreement on their importance as compared with selection, drift, and other such factors in shaping evolutionary history. This review distinguishes between "universal" and "local" constraints; it deals primarily with the latter, which apply to a limited range of taxa. Such constraints, typically, can be broken even within the taxa to which they apply, though with varying degrees of difficulty. The origin of constraints is discussed, five distinctive of constraint being explicitly considered. Three means of identifying constraints are set forth, as well as four means of distinguishing developmental from selective constraints. None of the latter (use of a priori adaptive predictions, direct measurement of selection, direct measurement of heritable variation, and use of the comparative method) is foolproof. In the final section, three larger issues regarding the role of developmental constraints in evolution are discussed: the extent to which evolutionary stasis can be explained in developmental terms, the extend to which evolutionary trends and patterns might be a consequence of developmental constraints, and the extent to which various genetic and developmental mechanisms have evolved in virtue of the need of lineages to manifest evolutionary plasticity (or adaptability) if they are to survive. Although no definitive conclusions are reached on these larger issues, we bring recent advances in developmental biology, evolutionary theory, and (to a limited extent) molecular biology to bear on them.},
	pages = {265--287},
	number = {3},
	journaltitle = {The Quarterly Review of Biology},
	author = {Smith, J. Maynard and Burian, R. and Kauffman, S. and Alberch, P. and Campbell, J. and Goodwin, B. and Lande, R. and Raup, D. and Wolpert, L.},
	urldate = {2021-06-29},
	date = {1985-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Snapshot:/home/fabien/Zotero/storage/YP2ZVNKF/414425.html:text/html},
}

@incollection{james_lecture_1907,
	location = {New York, {NY}, {US}},
	title = {Lecture {II}: What pragmatism means},
	shorttitle = {Lecture {II}},
	abstract = {The meaning of pragmatism is discussed. The pragmatic method is primarily a method of settling metaphysical disputes that otherwise might be interminable. This method tries to interpret such notions as determinism versus freedom and materialism versus spiritualism by tracing their practical consequences. What difference would it practically make to any one if this notion rather than that notion were true? If no practical difference whatever can be traced, then the alternatives mean practically the same thing, and all dispute is idle. The history of the method and its character and affinities are discussed, as well as how it contrasts with rationalism and intellectualism. The word pragmatism has come to be used in a wider sense, as meaning also a certain theory of truth. In this sense, pragmatism is equivalent to humanism, which views truth as giving human satisfaction in marrying previous parts of experience with newer parts. As a mediator between empiricism and rationalism, pragmatism has no prejudices, no obstructive dogmas, and no rigid canons of what shall count as proof. Rationalism sticks to logic and the empyrean. Empiricism sticks to the external senses. Pragmatism is willing to take anything, to follow either logic or the senses and to count the humblest and most personal experiences. Its only test of probable truth is what fits every part of life best and combines with the collectivity of experience's demands, nothing being omitted. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {43--81},
	booktitle = {Pragmatism: A new name for some old ways of thinking},
	publisher = {Longmans, Green and Co},
	author = {James, William},
	date = {1907},
	doi = {10.1037/10851-002},
	keywords = {Intellectualism, Metaphysics, Philosophies, Positivism (Philosophy), Pragmatism, Reasoning},
	file = {Snapshot:/home/fabien/Zotero/storage/QE8UF8Z4/2005-12954-002.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2021-06-30},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/fabien/Zotero/storage/ELCUAQ9A/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/fabien/Zotero/storage/STWL6MA7/1706.html:text/html},
}

@article{moore_progress_2006,
	title = {Progress in digital integrated electronics [Technical literaiture, Copyright 1975 {IEEE}. Reprinted, with permission. Technical Digest. International Electron Devices Meeting, {IEEE}, 1975, pp. 11-13.]},
	volume = {11},
	issn = {1098-4232},
	doi = {10.1109/N-SSC.2006.4804410},
	abstract = {This article is reprinted from the Internaional Electron Devices Meeting (1975). It discusses the complexity of integrated circuits, identifies their manufacture, production, and deployment, and addresses trends to their future deployment.},
	pages = {36--37},
	number = {3},
	journaltitle = {{IEEE} Solid-State Circuits Society Newsletter},
	author = {Moore, Gordon E.},
	date = {2006-09},
	note = {Conference Name: {IEEE} Solid-State Circuits Society Newsletter},
	keywords = {Geometry, Approximation methods, Arrays, Complexity theory, Integrated circuits, Probability density function},
	file = {IEEE Xplore Abstract Record:/home/fabien/Zotero/storage/VRYKYQQG/4804410.html:text/html},
}

@article{kohonen_self-organized_1982,
	title = {Self-organized formation of topologically correct feature maps},
	volume = {43},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00337288},
	doi = {10.1007/BF00337288},
	abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	pages = {59--69},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Kohonen, Teuvo},
	urldate = {2021-07-06},
	date = {1982-01-01},
	langid = {english},
}

@article{aizerman_theoretical_1964,
	title = {Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning},
	volume = {25},
	url = {https://ci.nii.ac.jp/naid/10021200712/},
	pages = {821--837},
	journaltitle = {Automation and Remote Control},
	author = {{AIZERMAN}, M. A.},
	urldate = {2021-07-06},
	date = {1964},
	file = {Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning Snapshot:/home/fabien/Zotero/storage/VHHISFLU/10021200712.html:text/html},
}

@article{stengel_human_1965,
	title = {Human Growth and the Development of Personality},
	volume = {2},
	issn = {0007-1447},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1846596/},
	pages = {1232},
	number = {5472},
	journaltitle = {British Medical Journal},
	shortjournal = {Br Med J},
	author = {Stengel, E.},
	urldate = {2021-07-09},
	date = {1965-11-20},
	pmid = {null},
	pmcid = {PMC1846596},
	file = {PubMed Central Full Text PDF:/home/fabien/Zotero/storage/N5KZBSNS/Stengel - 1965 - Human Growth and the Development of Personality.pdf:application/pdf},
}

@article{pearlmutter_gradient_1995,
	title = {Gradient calculations for dynamic recurrent neural networks: a survey},
	volume = {6},
	issn = {1941-0093},
	doi = {10.1109/72.410363},
	shorttitle = {Gradient calculations for dynamic recurrent neural networks},
	abstract = {Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some "tricks of the trade" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed.{\textless}{\textgreater}},
	pages = {1212--1228},
	number = {5},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Pearlmutter, B.A.},
	date = {1995-09},
	note = {Conference Name: {IEEE} Transactions on Neural Networks},
	keywords = {Computational complexity, History, Machine learning, Computational modeling, Neural networks, Backpropagation algorithms, Clocks, Equations, Output feedback, Recurrent neural networks},
	file = {Version acceptée:/home/fabien/Zotero/storage/Q5D7MDLF/Pearlmutter - 1995 - Gradient calculations for dynamic recurrent neural.pdf:application/pdf;IEEE Xplore Abstract Record:/home/fabien/Zotero/storage/FFRXFFUH/410363.html:text/html},
}

@article{poli_parallel_1996,
	title = {Parallel Distributed Genetic Programming},
	url = {https://core.ac.uk/display/24352294},
	abstract = {This paper describes Parallel Distributed Genetic Programming ({PDGP}), a new form of Genetic Programming ({GP}) which is suitable for the development of programs with a high degree of parallelism and an efficient and effective reuse of partial results. Programs are represented in {PDGP} as graphs with nodes representing functions and terminals, and links representing the flow of control and results. In the simplest form of {PDGP} links are directed and unlabelled, in which case {PDGP} can be considered a generalisation of standard {GP}. However, more complex (direct) representations can be used, which allow the exploration of a large space of possible programs including standard tree-like programs, logic networks, neural networks, recurrent transition networks, finite state automata, etc. In {PDGP}, programs are manipulated by special crossover and mutation operators which guarantee the syntactic correctness of the offspring. For this reason {PDGP} search is very efficient. {PDGP} programs can be execu..},
	author = {Poli, Riccardo},
	urldate = {2021-07-12},
	date = {1996},
	langid = {english},
	note = {Publisher: {McGraw}-Hill},
	file = {Snapshot:/home/fabien/Zotero/storage/6TXVT8L8/24352294.html:text/html},
}

@book{holmes_john_2014,
	location = {London},
	edition = {2},
	title = {John Bowlby and Attachment Theory},
	isbn = {978-1-315-87977-2},
	abstract = {Second edition, completely revised and updated 
John Bowlby is one of the outstanding psychological theorists of the twentieth century. This new edition of John Bowlby and Attachment Theory is both a biographical account of Bowlby and his ideas and an up-to-date introduction to contemporary attachment theory and research, now a dominant force in psychology, counselling, psychotherapy and child development.
Jeremy Holmes traces the evolution of Bowlby’s work from a focus on delinquency, material deprivation and his dissatisfaction with psychoanalysis's imperviousness to empirical science to the emergence of attachment theory as a psychological model in its own right. This new edition traces the explosion of interest, research and new theories generated by Bowlby’s followers, including Mary Main’s discovery of Disorganised Attachment and development of the Adult Attachment Interview, Mikulincer and Shaver’s explorations of attachment in adults and the key contributions of Fonagy, Bateman and Target. The book also examines advances in the biology and neuroscience of attachment.
Thoroughly accessible yet academically rigorous, and written by a leading figure in the field, John Bowlby and Attachment Theory is still the perfect introduction to attachment for students of psychology, psychiatry, counselling, social work and nursing.},
	pagetotal = {272},
	publisher = {Routledge},
	author = {Holmes, Jeremy and Holmes, Jeremy},
	date = {2014-01-13},
	doi = {10.4324/9781315879772},
}

@inproceedings{reynolds_competition_1994,
	title = {Competition, Coevolution and the Game of Tag},
	abstract = {Tag is a children's game based on symmetrical pursuit and evasion. In the experiments described here, control programs for mobile agents (simulated vehicles) are evolved based on their skill at the game of tag. A player's fitness is determined by how well it performs when placed in competition with several opponents chosen randomly from the coevolving population of players. In the beginning, the quality of play is very poor. Then slightly better strategies begin to exploit the weaknesses of others. Through evolution, guided by competitive fitness, increasingly better strategies emerge over time. 1. Introduction Many of us remember playing the game of tag as children. Tag is played by two or more, one of whom is designated as it. The it player chases the others, who all try to escape. Tag is a simple contest of pursuit and evasion. These activities are common in the natural world, most predatorprey interactions involve pursuit and evasion. Tag also includes an aspect of role-reversal, b...},
	pages = {59--69},
	publisher = {{MIT} Press},
	author = {Reynolds, Craig W.},
	date = {1994},
	file = {Citeseer - Full Text PDF:/home/fabien/Zotero/storage/VV9C8B8U/Reynolds - 1994 - Competition, Coevolution and the Game of Tag.pdf:application/pdf;Citeseer - Snapshot:/home/fabien/Zotero/storage/NR5YVEF8/download.html:text/html},
}

@article{fjelland_why_2020,
	title = {Why general artificial intelligence will not be realized},
	volume = {7},
	rights = {2020 The Author(s)},
	issn = {2662-9992},
	url = {https://www.nature.com/articles/s41599-020-0494-4},
	doi = {10.1057/s41599-020-0494-4},
	abstract = {The modern project of creating human-like artificial intelligence ({AI}) started after World War {II}, when it was discovered that electronic computers are not just number-crunching machines, but can also manipulate symbols. It is possible to pursue this goal without assuming that machine intelligence is identical to human intelligence. This is known as weak {AI}. However, many {AI} researcher have pursued the aim of developing artificial intelligence that is in principle identical to human intelligence, called strong {AI}. Weak {AI} is less ambitious than strong {AI}, and therefore less controversial. However, there are important controversies related to weak {AI} as well. This paper focuses on the distinction between artificial general intelligence ({AGI}) and artificial narrow intelligence ({ANI}). Although {AGI} may be classified as weak {AI}, it is close to strong {AI} because one chief characteristics of human intelligence is its generality. Although {AGI} is less ambitious than strong {AI}, there were critics almost from the very beginning. One of the leading critics was the philosopher Hubert Dreyfus, who argued that computers, who have no body, no childhood and no cultural practice, could not acquire intelligence at all. One of Dreyfus’ main arguments was that human knowledge is partly tacit, and therefore cannot be articulated and incorporated in a computer program. However, today one might argue that new approaches to artificial intelligence research have made his arguments obsolete. Deep learning and Big Data are among the latest approaches, and advocates argue that they will be able to realize {AGI}. A closer look reveals that although development of artificial intelligence for specific purposes ({ANI}) has been impressive, we have not come much closer to developing artificial general intelligence ({AGI}). The article further argues that this is in principle impossible, and it revives Hubert Dreyfus’ argument that computers are not in the world.},
	pages = {1--9},
	number = {1},
	journaltitle = {Humanities and Social Sciences Communications},
	shortjournal = {Humanit Soc Sci Commun},
	author = {Fjelland, Ragnar},
	urldate = {2021-07-14},
	date = {2020-06-17},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Palgrave
Subject\_term: Ethics;Science, technology and society
Subject\_term\_id: ethics;science-technology-and-society},
	file = {Full Text PDF:/home/fabien/Zotero/storage/4BSSWGUU/Fjelland - 2020 - Why general artificial intelligence will not be re.pdf:application/pdf},
}

@article{bak_adaptive_2001,
	title = {Adaptive learning by extremal dynamics and negative feedback},
	volume = {63},
	issn = {1539-3755},
	doi = {10.1103/PhysRevE.63.031912},
	abstract = {We describe a mechanism for biological learning and adaptation based on two simple principles: (i) Neuronal activity propagates only through the network's strongest synaptic connections (extremal dynamics), and (ii) the strengths of active synapses are reduced if mistakes are made, otherwise no changes occur (negative feedback). The balancing of those two tendencies typically shapes a synaptic landscape with configurations which are barely stable, and therefore highly flexible. This allows for swift adaptation to new situations. Recollection of past successes is achieved by punishing synapses which have once participated in activity associated with successful outputs much less than neurons that have never been successful. Despite its simplicity, the model can readily learn to solve complicated nonlinear tasks, even in the presence of noise. In particular, the learning time for the benchmark parity problem scales algebraically with the problem size N, with an exponent k approximately 1.4.},
	pages = {031912},
	number = {3},
	journaltitle = {Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics},
	shortjournal = {Phys Rev E Stat Nonlin Soft Matter Phys},
	author = {Bak, P. and Chialvo, D. R.},
	date = {2001-03},
	pmid = {11308683},
	keywords = {Neurons, Action Potentials, Adaptation, Physiological, Computer Simulation, Feedback, Learning, Models, Neurological, Nerve Net, Nonlinear Dynamics, Stochastic Processes, Synaptic Transmission},
	file = {Version soumise:/home/fabien/Zotero/storage/SYZDKUXE/Bak et Chialvo - 2001 - Adaptive learning by extremal dynamics and negativ.pdf:application/pdf},
}

@article{ohman_fears_2001,
	title = {Fears, phobias, and preparedness: Toward an evolved module of fear and fear learning},
	volume = {108},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/0033-295X.108.3.483},
	shorttitle = {Fears, phobias, and preparedness},
	abstract = {An evolved module for fear elicitation and fear learning with 4 characteristics is proposed. (a) The fear module is preferentially activated in aversive contexts by stimuli that are fear relevant in an evolutionary perspective. (b) Its activation to such stimuli is automatic. (c) It is relatively impenetrable to cognitive control. (d) It originates in a dedicated neural circuitry, centered on the amygdala. Evidence supporting these propositions is reviewed from conditioning studies, both in humans and in monkeys; illusory correlation studies; studies using unreportable stimuli; and studies from animal neuroscience. The fear module is assumed to mediate an emotional level of fear learning that is relatively independent and dissociable from cognitive learning of stimulus relationships. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {483--522},
	number = {3},
	journaltitle = {Psychological Review},
	author = {Öhman, Arne and Mineka, Susan},
	date = {2001},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Neurosciences, Animal Emotionality, Animal Models, Cognitive Ability, Emotional Control, Fear},
}

@incollection{rescorla_computational_2020,
	edition = {Fall 2020},
	title = {The Computational Theory of Mind},
	url = {https://plato.stanford.edu/archives/fall2020/entries/computational-mind/},
	abstract = {Could a machine think? Could the mind itself be a thinking machine?The computer revolution transformed discussion of these questions,offering our best prospects yet for machines that emulate reasoning,decision-making, problem solving, perception, linguisticcomprehension, and other mental processes. Advances incomputing raise the prospect that the mind itself is a computationalsystem—a position known as the computational theory ofmind ({CTM}). Computationalists are researchers whoendorse {CTM}, at least as applied to certain important mentalprocesses. {CTM} played a central role within cognitive science duringthe 1960s and 1970s. For many years, it enjoyed orthodox status. Morerecently, it has come under pressure from various rival paradigms. Akey task facing computationalists is to explain what one means whenone says that the mind “computes”. A second task is toargue that the mind “computes” in the relevant sense. Athird task is to elucidate how computational description relates toother common types of description, especially neurophysiologicaldescription (which cites neurophysiological properties of theorganism’s brain or body) and intentional description(which cites representational properties of mental states).},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Rescorla, Michael},
	editor = {Zalta, Edward N.},
	urldate = {2021-07-19},
	date = {2020},
	keywords = {connectionism, analogy and analogical reasoning, anomalous monism, causation: the metaphysics of, Chinese room argument, Church-Turing Thesis, cognitive science, computability and complexity, computation: in physical systems, computer science, philosophy of, computing: modern history of, culture: and cognitive science, externalism about the mind, folk psychology: as mental simulation, frame problem, functionalism, Gödel, Kurt, Gödel, Kurt: incompleteness theorems, Hilbert, David: program in the foundations of mathematics, language of thought hypothesis, mental causation, mental content: causal theories of, mental content: narrow, mental content: teleological theories of, mental imagery, mental representation, mental representation: in medieval philosophy, mind/brain identity theory, models in science, multiple realizability, other minds, reasoning: automated, reasoning: defeasible, reduction, scientific, simulations in science, Turing machines, Turing test, Turing, Alan, zombies},
	file = {SEP - Snapshot:/home/fabien/Zotero/storage/X6X4H6VI/computational-mind.html:text/html},
}

@article{oconnor_emergent_2002,
	title = {Emergent Properties},
	url = {https://stanford.library.sydney.edu.au/archives/win2019/entries/properties-emergent/},
	author = {O'Connor, Timothy and Wong, Hong Yu},
	urldate = {2021-07-19},
	date = {2002-09-24},
	note = {Last Modified: 2015-06-03},
	file = {Snapshot:/home/fabien/Zotero/storage/6443G9RS/properties-emergent.html:text/html},
}