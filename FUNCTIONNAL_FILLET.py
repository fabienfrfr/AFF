#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Aug  1 21:09:28 2021
@author: fabien
https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html
https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
https://pytorch.org/tutorials/beginner/nn_tutorial.html
"""

# ML module
import numpy as np, pylab as plt
import torch, torch.nn as nn
import torch.nn.functional as F

# networks construction
from GRAPH_EAT import GRAPH_EAT
from pRNN_GEN import pRNN

# calculation (multi-cpu) and data (namedtuple) optimisation
#import multiprocessing, collections

# control net
class CTRL_NET(nn.Module):
    def __init__(self, I,O):
        super(CTRL_NET, self).__init__()
        H = int(np.sqrt(I))
        self.IN = nn.Conv1d(I, I, 1, groups=I, bias=True)
        self.H1 = nn.Linear(I, H)
        self.H2 = nn.Linear(H, H)
        self.OUT = nn.Linear(H, O)

    def forward(self, x):
        s = x.shape
        x = F.relu(self.IN(x.view(s[0],s[1],1)).view(s))
        x = F.relu(self.H1(x))
        x = F.relu(self.H2(x))
        x = F.relu(self.OUT(x))
        return F.log_softmax(x, dim=1)

# ff module
class model():
    def __init__(self, IO, BATCH_SIZE, NB_GEN, NB_SEEDER, FITNESS = 0.1, LEARNING_RATE = 1e-6, MOMENTUM = 0.5):
        # Parameter
        self.BATCH_SIZE = BATCH_SIZE
        self.NB_GEN = NB_GEN
        self.NB_SEEDER = NB_SEEDER**2
        self.FITNESS = FITNESS
        self.LR = LEARNING_RATE
        self.MM = MOMENTUM
        # generate first ENN step
        self.GRAPH_LIST = [GRAPH_EAT([IO[0], IO[1], 1], None) for n in range(NB_SEEDER-1)]
        self.SEEDER_LIST = [CTRL_NET()]
        for g in self.GRAPH_LIST :
            NEURON_LIST = g.NEURON_LIST
            self.SEEDER_LIST += [pRNN(NEURON_LIST, BATCH_SIZE, IO[0])]
        # generate loss-optimizer
        self.LOSS = [nn.MSELoss(reduction='sum') for n in range(NB_SEEDER)]
        self.OPTIM = [torch.optim.SGD(s.parameters(), lr=LEARNING_RATE,momentum=MOMENTUM) for s in self.SEEDER_LIST]
        # scoring
        self.SCORE = []
        # for next gen (n-plicat) and control group
        self.NB_CONTROL = 1 # always (preference)
        self.NB_CHALLENGE = int(np.sqrt(self.NB_SEEDER)-self.NB_CONTROL)
        self.NB_SURVIVOR = self.NB_CHALLENGE # square completion
        self.NB_CHILD = int(np.sqrt(self.NB_SEEDER)-1)
        
    def fit(self, DATA, LABEL):
        # Store data/label
        self.DATA = DATA
        self.LABEL = LABEL
        # Training Loop :
        for i in range(self.NB_GEN):
            for j in range(self.NB_SEEDER):
                # compile and collect score
                self.SCORE += [self.compile_(10,)]
                # update Progress bar
                print(i,j)
            # update seeder list
            self.evolution()
        # ending
        print('PROCESS FINISH')
    
    def compile_(self, epochs, train_dl, criterion, optimizer) :
        # iterate through all the epoch
        for epoch in range(epochs):
            # go through all the batches generated by dataloader
            for i, (inputs, targets) in enumerate(train_dl):
                    # clear the gradients
                    optimizer.zero_grad()
                    # compute the model output
                    yhat = model(inputs)
                    # calculate loss
                    loss = criterion(yhat, targets.type(torch.LongTensor))
                    # credit assignment
                    loss.backward()
                    # update model weights
                    optimizer.step()
        # evaluate accuracy_score
        """
        correct += (predicted == labels).sum().item()
        accuracy = 100 * float(correct_count) / total_pred[classname]
        """
        score = 1 #accuracy/100
        # return score
        return score
    
    def evolution(self):
        # new data
        GRAPH_LIST_NEW = []
        SEEDER_LIST_NEW = []
        LOSS_NEW = []
        OPTIM_NEW = []
        # choose best
        BEST = np.argsort([10,20,1])[::-1]
        NB_SELCTD = int(BEST.size*self.FITNESS)
        NB_MUT = 10
        NB_CHALLENGER = 10
        # env selection
        for i in range(NB_SELCTD):
            idx = BEST[i]
            # survivor
            GRAPH_LIST_NEW += [self.GRAPH_LIST[idx].NEXT_GEN(-1)]
            # mutation
            for j in range(NB_MUT):
                GRAPH_LIST_NEW += [self.GRAPH_LIST[idx].NEXT_GEN()]
        # challenger
        GRAPH_LIST_NEW += [GRAPH_EAT([NB_SEEDER, IO[0], IO[1], 1], None) for n in range(NB_CHALLENGER)]
        # re-construct nn
        for g in GRAPH_LIST_NEW :
            NEURON_LIST = g.NEURON_LIST
            SEEDER_LIST_NEW += [pRNN(NEURON_LIST, BATCH_SIZE, IO[0])]
        # generate loss-optimizer
        self.LOSS = [nn.MSELoss(reduction='sum') for n in range(NB_SEEDER)]
        self.OPTIM = [torch.optim.SGD(s.parameters(), lr=1e-6) for s in self.SEEDER_LIST]
        # init scoring
        self.SCORE = []
            
        
    def predict(self, DATA_NEW):
        # Put data in best model
        self.LABEL_NEW = self.SEEDER_LIST[-1](DATA_NEW)
        print(self.SEEDER_LIST[-1])
        # return result
        return self.LABEL_NEW       

### TESTING PART
if __name__ == '__main__' :
    # module for test
    from tensorflow.keras.datasets import mnist
    # import mnist
    (X_train_data,Y_train_data),(X_test_data,Y_test_data) = mnist.load_data()
    plt.imshow(X_train_data[0]); plt.show(); plt.close()
    # data info
    N, x, y = X_train_data.shape
    f = np.unique(Y_train_data).size
    # init
    model = model([x*y,f],10,10,10)
    # test
    XTEST = torch.tensor(X_train_data[0].reshape(-1)[None], dtype=torch.float)
    out = model.SEEDER_LIST[0](XTEST); print(out)
    # training
    model.fit(X_train_data,Y_train_data)
    # predict
    X_torch = torch.tensor(X_test_data[0].reshape((-1,))[np.newaxis], dtype=torch.float)
    result = model.predict(X_torch)